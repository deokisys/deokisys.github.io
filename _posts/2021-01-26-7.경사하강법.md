---
layout: post
title: 7.경사하강법
categories: AI boost
tags: [ai,boost,SGD,경사하강법,경사상승법,미분,선형회귀,]
toc : true
math: true
---

# 강의 복습 내용
- 기본 미분
- 경사상승법과 경사 하강법
- 확률적 경사 하강법(stochastic gradient descent)SGD
- 딥러닝에서 사용되는방법


## 얻은 지식
- 미분, 편미분
- 경사하강법
  - 베타란?
- SGD(stochastic gradient descent)

-----

## 미분
- **미분** 변수의 움직임에 따른 함수값이 변화량을 측정
  - 기울기로 알고있음
- `sympy.diff`로 구할수 있다.
- $$f(x)= x^2 + 2x+ 3 $$의 미분 $$ f(x)' $$를 구하기

```python
import sympy as sym
from sympy.abc import x

# x에 대해 미분
sym.diff(sym.poly(x**2 + x*2 + 3),x)
```

- 함수 $$ f $$의 주어진점 (x,f(x))에서의 **접선의 기울기**를 구할 수 있다.
  - (x,f(x))와 h만큼 떨어진 (x+h,f(x+h))가 있으면 기울기 공식 f(x+h)-f(x) / h가 기울기가 된다.
  - h를 0으로 수렴시키면 x에서의 접선의 기울기를 구할 수 있다.

### 미분의 사용법
- 기울기를 통해 어느방향으로 가면 함수값이 증가할지 감소할지 알 수 있다.
  - 이를 통해 최적화를 편하게 사용한다.
- 증가
  - 함수값 증가를 위해서는 미분값을 더해주면 증가한다.
  - 미분값이 **음수**이면 미분값을 더해주면, 해당 좌표에서 **좌측**으로 이동하게 되면서 함수값이 증가한다.
  - 미분값이 **양수**이면 미분값을 더해주면, 해당 좌표에서 **우측**으로 이동하게 되면서 함수값이 증가한다.
  - `경사 상승법(gradient ascent)`이라 한다.
    - 극대값의 위치를 구할때 사용한다.
- 감소
  - 함수값 감소를 위해서는 미분값을 빼주면 감소한다.
  - 미분값이 **음수**이면 미분값을 빼주면, 해당 좌표에서 **우측**으로 이동하게 되면서 함수값이 감소한다.
    - 음수이므로 뺄셈이 덧셈으로 변한다.
  - 미분값이 **양수**이면 미분값을 빼주면, 해당 좌표에서 **좌측**으로 이동하게 되면서 함수값이 감소한다. 
  - `경사 하강법(gradient decent)`이라 한다.
    - 극소값의 위치를 구할때 사용한다.

- `경사 상승법`,`경상 하강법`은 극값에 도달하면 움직임을 멈춘다.
  - 미분된 값이 0이 되서 움직임이 없어진다.

### 경사 하강법 알고리즘 예

```python
#gradient = 미분계산
#init = 시작
#lr = learning rate
#eps = 종료조건

var = init
grad = gradient(var) #기울기계산
while(abs(grad) > abs):
  var = var - lr*grad
  grad = gradient(var)
```

- 종료조건
  - 미분이 0이 되야 하지만 정확한 0이 나오지 않음
  - 적절한 `eps`를 정하여 eps보다 작을때 종료되도록 한다.
- 학습속도  
  - lr(learning rate)를 통해 경사하강법 속도를 조절할 수 있다.
  - 학습률은 미분의 업데이트 속도가 되며 적절한 수치를 줘야한다.

### 벡터인경우
- 변수가 x로 하나가 아닌 다변수 함수의 경우 `편미분(partial differentiation)`을 사용
  - 하나의 변수에대해서 미분을 진행
  - 다른 변수들은 상수 취급하는 미분
- 각 변수 별로 편미분을 계산한 `그레디언트 벡터`를 이용하여 경사하강/상승법에 사용
  - $$ \nabla f = (...)$$ nabla라는 기호를 표시 
- $$ -\nabla f $$음수는 각 점에서 감소하게 되는 방향가며 최솟값을 찾을 수 있다
- $$ \nabla f $$양수는 각 점에서 가장빨리 증가하는 방향으로 간다.


## 경사하강법 매운맛

### 선형회귀분석
- `np.linalg.pinv`를 이요하면 데이터를 선형 모델로 해성하는 선형회귀식 찾는것
  - n개의 데이터가 구성되었을때
  - 데이터를 잘 표현하는 선형모델 찾을 때 사용
  - 변수의 갯수보다 식이 더 많을때
- 무어-펠로즈 역행렬을 사용하지 않고 경사하강법으로 선형모델 찾기

### 경사하강법
- 전제
  - 아래와 같은 식이 존재

$$
\begin{matrix}
x_{11}\beta_1+x_{12}\beta_2+ \cdots +x_{1m}\beta_m = y_1 \\
x_{21}\beta_1+x_{22}\beta_2+ \cdots +x_{1m}\beta_m = y_2 \\
\vdots \\ 
x_{n1}\beta_1+x_{n2}\beta_2+ \cdots +x_{nm}\beta_m = y_n \\
\end{matrix}

$$

  - 설명하면 X라는 주어진값과 그에 따른 결과 y가있으며, b는 구하고자 하는 해이다.
    - 새로운 X가 주어질때 y를 예측해야한다.
  - X는 여러 점을 의미하고, b를 계수로 사용하여 만드는 직선 Xb를 만들어낸다.
  - 이때 어떤 b를 써야 좌표의 점들을 잘 표현할 수 있는지 찾아야한다.

$$
\begin{bmatrix}
x_{11},x_{12}, \cdots ,x_{1m} \\
x_{21},x_{22}, \cdots ,x_{1m} \\
\vdots \\ 
x_{n1},x_{n2}, \cdots ,x_{nm} \\
\end{bmatrix}
\begin{bmatrix}
\beta_1 \\
\beta_2 \\
\vdots \\ 
\beta_m \\
\end{bmatrix}
=
\begin{bmatrix}
y_1 \\
y_2 \\
\vdots \\ 
y_m \\
\end{bmatrix}
$$

- 선형회귀의 목적식
  - 목적식 $$\|y-X\beta\|_2$$ 로 주어진 정답 y, 선형모델 $$ X\beta$$의 L2노름을 최소화하는것
  - 이를 최소화 하는 $$ \beta $$를 찾는것
- 그레디언트 벡터
  - $$ \nabla_{\beta}\|y-X\beta\|_2 = ({\partial}_{\beta_1}\|y-X\beta\|_2, \cdots , {\partial}_{\beta_d}\|y-X\beta\|_2) $$로 계산된다.
    - 그레디언트 벡터는 각 $$\beta$$마다 미분한 벡터로 이루어진다.
    - $$\beta_1$$, $$\beta_2$$,...$$\beta_d$$로 미분한다.
    - ![식 이미지](https://user-images.githubusercontent.com/24247768/105827899-e0598c00-6005-11eb-8bde-3b3660cbb07f.png)
      - 좌측의 L2부터 진행하면 우측처럼 정리 할 수 있다.
      - L2는 유클리드처럼 모든 요소를 제곱하고 더한값을 제곱근한 값이다.
        - 기존 L2계산과 약간 다름 
        - L2는 n개의 데이터를 가지고 계산됨, i부터 n까지의 결과값에대해 제곱하여 더한 값을 평균한 이후 제곱근한다.
          - 중간에 1/n이 있다.
        - 내부는 $$(y1-(x_{11}\beta_1+x_{12}\beta_2+...x_{1d}\beta_d))^2 + ...$$로 진행된다.
          - 각 결과 y에서 $$\beta$$로 계산된 $$X\beta$$결과를 뺀것을 제곱하여 더해주는것
  - 식을 풀면 $$ -  \frac{X^T(y-X\beta)}{n\|y-X\beta\|_2} $$
    - 복잡해 보여도 $$X\beta$$를 계수$$\beta$$에 대해 미분한 $$X^T$$만 곱한것
  - $$ \nabla_{\beta}\|y-X\beta\|_2^2 $$로 제곱을 해주면 식이 좀더 간단해진다.
    - $$ - \frac{2}{n}X^T(y-X\beta) $$로 간결화해진다.
  - 목적식을 최소화 하는 $$\beta$$를 구하는 경사하강법 알고리즘은
    - $$ \beta^{(t+1)} \leftarrow \beta^{(t)} - (- \frac{2\lambda}{n}X^T(y-X\beta^{(t)}))$$
    - $$ \lambda$$는 learning rate가 되고 경사 하강법으로 -(빼기)이지만 기울기가 -이므로 +로 계산된것이다.

```python
X=np.arrray([[1,1],[1,2],[2,2],[2,3]])
#[1,1]
#[1,2]
#[2,2]
#[2,3]
y = np.dot(X,np.array([1,2]))+3
#[6,8,9,11]
#X는 입력, y는 결과

beta_gd = [10.1,15.1,-6.5]
# beta에 초기 랜덤하게 값을 넣어준다.
# 이 beta값을 구하는게 목적

X_ = np.array([np.append(x,1) for x in X])
#[1,1,1]
#[1,2,1]
#[2,2,1]
#[2,3,1]
#-------
for t in range(5000):#5000번 반복
  error = y - X_ @ beta_gd
  #y는 정답
  # X_@beta_gd는 beta_gd로 구한 예측값
  #둘의 차이를 error로 한다.

  grad = -np.transpose(X_) @ error
  beta_gd  = beta_gd - 0.01 * grad
  #beta_gd를 업데이트
  #0.01의 학습률로 적용된다.


```

- 학습률과 학습횟수가 중요하다
- 만능이아니며 볼록(convex)한 함수에 대해서 수렴이 보장된다.
- **비선형회귀** 문제는 목적식이 볼록하지 않을수 있으므로, 수렴이 항상 보장되지는 않다.
  - SGD를 사용한다.

### 확률적 경사하강법SGD
- SGD(stochastic gradeind descent)
- 비선형회귀일경우
- 볼록이 아닌 (non-convex)일때
- 모든 데이터가 아닌, 일부를 활용하는것
- 한개는 SGD 일부는 minibatch SGD라고 부른다.
- 일부를 사용하기 때문에 연산자원을 좀더 효율적
- 미니 배치를 사용하기 때문에 전체 데이터와 같지않은, 유사한 그레디언트 벡터를 계산할 수 있다.
  - 매번 다른 미니배치를 사용하기 때문에 목적식 모양이 바뀐다.
  - 매번 다른 모양이지만 방향은 같을거란 기대가 있다.
  - 모든 미니배치의 목적식에서 0이거나 0이 아닌경우가 나오기 때문에 non-convex일때 지역적인 최소값에서 벗어 날 수 있게 된다.
- 정확한 그레디언트 벡터를 계산한게 아니어서 확률적으로 움직이듯 보인다.
  - 실제 최소로 가기보단 왔다갔다 거리면서 움직이듯 보인다.
  - 결론적으로는 최소값을 찾는것
- 경사하강법보다 빠르게 움직인다.
  - 미니배치로 그레디언트 벡터를 계산하기 때문에
  - 정확하진 않아도 빠르다.


----

## 좀더 찾아보기
- sympy
- 성김 역전파 영상
- 기호
  - [참고](https://librewiki.net/wiki/수학_기호)
  - $$ \mathcal{L} $$ 라플라스 변환
  - $$ \hat{y} $$ ^표시 = 추정량
  - $$ \mathbb{E} $$ 기대값


-----

## 피어세션 정리
- 오늘 배운 경사하강법 관련 배경 지식이 어렵다.
- 기호 정리


## 의문사항
- 행과 열 관계에 대해 이해하기 어렵다.
  - 행<열이면 연립방정식의 해
  - 행>열이면 선형회귀식
  - 어째서?
- 경사하강법 알듯하면서 애매하게 의문인 부분확인
  - 잘 알고있는지 완벽히 이해하지 못함
