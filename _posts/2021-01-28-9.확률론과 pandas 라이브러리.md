---
layout: post
title: 9.확률론과 pandas 라이브러리
categories: AI boost
tags: [ai,boost,pandas,groupby, pivot_table, join, database connection, xls persitence, 확률론, 몬테카를로]
toc : true
math: true
---

# 강의 복습 내용
- pandas의 라이브러리
  - groupby
  - pivot_table
  - join method
  - database connection
  - xls persistense


## 얻은 지식
- pandas 함수.
- 딥러닝의 학습방법.

----
## pandas library
- 수학, 코드에서 멈추지말아야 한다.

### group by
- sql에서의 groub by와 같음
  - 특정 index에 맞게 그룹화(split)
  - 그룹된index들 끼리 연산(apply)
  - 연산 결과를 다시 테이블로 합치기(combine)
- `df.groupby("묶음기준 column")["연산이 적용되는 column"].적용받는 연산`
  - `df.groupby("Team")["Points"].sum()`
    - Team이라는 컬럼을 기준으로 값들을 묶는다, 묶일때 points라는 컬럼의 데이터를 sum해준다.
  - `df.groupby(["Team","Year"])["Points"].sum()`
    - 두가지 컬럼을 묶는것도 가능.
    - 두개의 index가 생성된다.
    - `hierarchical index`라고 한다.

### hierarchical index
- 두개의 column을 groupby할 경우, index가 두개 생성된것
- 타입은 Series이다.
- `h_index[시작인덱스명:끝인덱스명]`
  - 일반적인 indexing도 가능
- `h_index.unstack()`
  - 매트릭스 형태로 바꿔준다.
  - 2번 index가 column으로 바뀐다.
- `h_index.reset_index()`
  - 그룹한 모습을 풀어준다.
- `h_index.swaplevel()`
  - index1과 index2를 서로 바꾼다.
- `h_index.sortlevel(level=0)`
  - 지정한 level의 index를 기준으로 정렬을 진행한다.
  - `h_index.sort_index(level=0)`과 같은 의미
- `h_index.sort_values()`
  - 결과값을 기준으로 정렬해준다.
- `h_index.std(level=1)`
  - std,sum같은 기본 연산 수행도 가능하다.
  - 해당 index 레벨을 기준으로, 결과값들을 계산한다.

### grouped
- groupby에 의해 split된 상태에서 추출 가능
- `groubed = df.groupby("Team")`
  - list로 변환 하여 볼수도 있음

```python
groubed = df.groupby("Team")
for name,group in grouped:
  print(name)
  print(group)
```

  - 코드처럼 grouped된것을 따로 출력하여 볼 수 있음
  - group의 타입은 DataFrame이다.
- `grouped.get_group("특정그룹 이름")`
  - 그룹된것에서 특정그룹을 따로 추출하는 함수
  - `grouped.get_group("Devils")`는 Team을 기준으로 Devils라는 그룹을 추출하라는 의미

- apply함수를 적용 가능
  - aggregation:요약된 통계정보를 추출
  - transformation:정보를 변환
  - filtration:특정 정보 제거
- Aggregation
  - 그룹마다 sum,max,mean을 적용
  - `grouped.agg(sum)`
    - 그룹내의 각 컬럼마다 해당 함수를 적용해준다.
    - 각 컬럼별 총합을 표시
  - `grouped.agg(max)`
    - 컬럼마다 최대인 값들만 표시한다.
- Transformation
  - 개별 데이터의 변환을 지원

```python
score = lambda x:(x.max())
grouped.transform(score)
"""
결과가 agg와는 다르게 압축된 결과가 아닌, 각 요소마다 해당 결과값을 넣어준다.

0 876 1 riders 2014
1 789 2 riders 2015
라는 데이터가 있을때
0 876 2 2015
1 876 2 2015
처럼 각각마다 해당 그룹에서 각 칼럼의 max인 값으로 바꿔준다.
"""

```

- Filter
  - 특정조건으로 데이터를 검색할 때 사용
  - `df.groupby("Team").filter(조건)`
  - `df.groupby("Team").filter(lambda x: x["Rank"].sum()>2)`
    - 그룹한것에서 Rank컬럼의 합이 2보다 큰것만 출력한다는 의미

### case study
-.....ㅓㅘㅣ호ㅓㅏㅣ호ㅓㅏㅣㅗㅓㅏ

### pivot table 
- 엑셀에서 보던것
- column에 추가로 labeling값을 추가
- value에 numeric type값을 aggregation하는 형태

```python
df_phone.pivot_table(["칼럼명"], # 실제 계산되는 값을가진 컬럼
                    index=[df_phone.컬럼명1,...] # 그룹화할 컬럼명
                    columns = df_phone.network # 컬럼이름
                    aggfunc = "sum" #컬럼마다 적용할 계산
                    fill_value = 0 # 없는값에 들어갈 기본값
)
```
  - 컬럼명에 들어간 값을 기준으로, index의 있는 컬럼으로 그룹화를 한다. 
  - 이후 column에 있는 컬럼명들로 묶어서 aggfunc을 진행한다.
  - 전체 테이블 모양은 index에는 index가 그룹되어 있고, column에는 column의 값들이 있는 테이블로 각 요소는 aggfunc이 적용된 모습
  - 복잡하다.

### crosstab
- 두 칼럼에 교차 빈도, 비율, 덧셈 등을 구할 때 사용
- pivot table의 특수한 형태
- user-item rating matrix등을 만들때 사용
  - 테이블 컬럼이 a,b,c라고 있을때 a를 세로축인 index로, b를 가로축인 column으로 , c를 결과값으로 할때 사용
  - pivot table과 같다.


### merge
- 두개의 데이터를 하나로 합침
  - 어떤 기준으로 합치기를 진행
- `pd.merge(df_a,df_b,on="subject_id")`
  - df_a와 df_b를 subject_id를 기준으로 합친다.
  - `pd.merge(df_a,df_b,left_on="subject_id1",right_on="subject_id2")`
  - df_a와 df_b의 기준이되는 컬럼이름이 다를때 left_on,right_on으로 특정한다.
- join
  - `pd.merge(df_a,df_b,on="subject_id", how='left')`
    - how에 left라고 하면 left를 기준으로 merge되고, 우측에 없는것은 NaN으로 된다.
  - `pd.merge(df_a,df_b,on="subject_id", how='right')`
  - `pd.merge(df_a,df_b,on="subject_id", how='outer')`
  - `pd.merge(df_a,df_b,on="subject_id", how='inner')`
- index를 기준으로 합치기
  - `pd.merge(df_a,df_b,right_index=True,left_index = True)`
    - 단순한 합치기가 된다.

### concat
- 값을 붙이는 연산
  - 아래로, 옆으로 붙이는게 있다.
  - append와 유사하다.

```python
df_new = pd.concat([df_a,df_b])
"""
   index subject_id first_name last_name
0    0     1          alex       andrson
"""

# index컬럼을 지워준다.
df_new.reset_index(drop=True)


"""
   subject_id first_name last_name
0     1        alex       andrson
"""
```


- `pd.concat([df_a,df_b],axis=1)`
  - axis가 0이면 아래로붙인다. 가로축기준으로
  - axis가 1이면 오른쪽으로 붙인다. 세로축기준



### persistence
- 데이터 입/출력
- db connection기능을 제공한다.
  - sql lite와 연결

```python
import sqlite3

conn = sqlite3.connect("./data/filights.db")
#db와 연결할 주소
cur = conn.cursor()
cur.execute("select * from airline limit 5;")#sql문 실행
results = cur.fetchall()# 결과 받아오기
```

- xls persistence
  - dataframe의 엑셀 추출 코드

```python
fdsafdsafdsafeafdsafdsa
```

- pickle persistence
fdsjaklf;djsak;fdsjaio 




## 확률론 맛보기
- 확률론 기반
- 손실함수(loss function)들의 작동원리
- 회귀분석
  - L2노름은 예측오차의 분산을 가장 최소화하는 방향
- 분류문제
  - 교차엔트로피(cross-entropy)는 모델예측의 불확실성을 최소화 하는 방향으로 학습

### 확률분포
- 데이터의 초상화
- 데이터를 해석하는데 중요한 도구

### 확률변수
- 확률분포에 따라구분 하는 종류가 있다.
- 이산형 확률변수
  - 확률변수가 가질 수 있는 경우의 수 를 모두 고려하여 확률을 더해서 모델링한다.
- 연속형 확률변수
  - 데이터 공간에 정의된 확률변수의 밀도위에서의 적분을 통해 모델링

### 조건부확률
- 조건부 확률 P(y|x)는 입력변수 x에 대해 정답이 y일 확률
  - 연속확률분포인경우 확률이 아닌 밀도로 해석
- 로지스틱회귀에서는 선형모델과 소프트맥스 함수의 결합
  - 데이터에서 추출된 패턴을 기반으로 확률를 해석하는데 사용
  - 분류에서는 선형모델을 통해 softmax로 확률 벡터를 얻을 수 있다.
    - softmax는 데이터로 추출된 특징 패턴을 pi$$\phi(x)$$라고 한다.
    - 이 특징 패턴을 통해 가중치행렬W와 행렬 곱을 통해 `주어진 x가 정답이 y일 확률`로 해석된다.
      - $$ P(y|\phi(x))$$라 써도 된다고 한다.
- 회귀문제인경우 조건부 기대값을 추적 $$E(y|x) $$
  - 조건부 기대값$$E\|y-f(x)\|_2 $$을 최소화 하는 함수 f(x)와 일치


### 기대값이란
- 확률분포가 주어지면 데이터를 분석하는 데 사용 가능한 여러 종류의 `통계적 범함수`(statistical functional)을 계산
- 기대값은 데이터를 대표하는 통계량
  - 동시에 확률 분포를 통해 다른 통계적 범함수를 계산하는데 사용
- 기대값은 대부분 평균이랑 동일한 개념으로 사용

### 몬테카를로




----

## 좀더 찾아보기
- lambda x:x가 숫자만 나오는이유는?
  - transform부분
- .plot으로 이미지화 할수있다.
- crosstab을 이용하는 이유
- sqlite3모듈

-----

## 피어세션 정리
- 발표 준비
- simpy가 좋다.
- softmax의 사용이유?
- 딥러닝 관계자와 이메일 불어봄
  - 핫한데 사람 대우가 많이 약하다는 의견


