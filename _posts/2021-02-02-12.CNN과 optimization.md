---
layout: post
title: 12.CNN과 optimization
categories: AI boost
tags: [ai,boost,CNN,optimization,gradient descent methd,regularization,adam,momentum]
toc : true
math: true
---

# 강의 복습 내용
CNN의 기초를 학습한다.
CNN의 역전파가 계산되느것을 학습한다.
최적화 관련 용어,gradient descent의 method인 adam, 학습을 방해하는 regularization들을 정리

## 얻은 지식
- CNN의 기본
- CNN의 역전파
- 최적화 용어
  - 
- gradient descent 방식
- regularization

----

## CNN
- convolution 연산과 다양한 차원에서의 연산방법
- convolution 연산의 역전파

### 기존
- 기존 다층신경망MLP은 각 뉴련들이 선형 모델과 활성함수로 모두 연결된 `fully connected`
  - 가중치w행은 행렬곱 형태이므로 w의 모든행,x의 모든열의 값을 사용한다.
  - 가중치의 크기도 커지고 학습도 커진다.

### CNN
- 고정된 커널(kernel)을 입력벡터 삭에서 움직이면서 선형모델과 합성함수가 적용되는 구조이다.
- 입력되는 x를 전부가 아니라 kernel크기만큼 사용한다.
- 고정된커널, 일부의x만 사용하므로 파라메터를 줄일수있다.

### 연산 이해
- 정의역이 연속인 공간에서 적분을 사용하여 정의
- 공간이 이산이면 국소
- 신호하는 G 커널 F
  - x-z는 신호텀, z커널텀 
- 수학적인 의미는 커널을 이용해서 국소적으로 증폭,감소
  - 정보를 추출
  - 정보를 필터링
- CNN은 빼기보다 더하기를 사용하므로 convolution이아닌 cross-correlation이라 부른다.
  - 관용적으로 convolution

- 커널은 정의역 내에서 움직여도 변하지 않는다.
  - translation invariant
- 주어진 신호에 국소적(local)으로 적용
  - 특정 지역에서만 작동한다.

- 이미지에서의 CNN
  - 이미지를 필터링을 하여, 블러처리, 경계선과 같은 처리를 한다.


### 다양한 차원의 CNN
- 1차원 뿐만 아니라 다양한 차원에서 계산이 가능
- 1차원은 하나의 좌표, 2차원은 2개의 좌표가 움직이는 방식이다.
- 음성, 자연어는 1차원
- 커널의 위치가 바뀌어도 커널은 그대로이다.

### 2차원 Convolution
- 커널을 행렬모양으로 사용
- 2차원은 x방향, y방향으로 움직이게 된다.
- 위치에 해당하는 p,q를 움직이면서 작동
- 행렬곱이 아닌 성분곱을 사용

- 출력크기를 예상할 수 있다.
  - 입력크기$$(H,W)$$커널크기$$K_H,K_W$$출력크기$$(O_H,O_W)$$
  - 높이= $$O_H = H-K_H+1$$
  - 넓이= $$O_W = W-K_W+1$$
  - 28*28입력에 3*3커널이면 출력은 26*26이 된다.

### 3차원 Convolution
- 3차원부턴 tensor
- 여러개의 2차원을 채널개수만큼 적용한다고 생각
- 3차원의 계산 결과는 2차원으로 출력된다.
  - $$(K_H,K_W,C) * (H,W,C) = (O_H,O_W,1)$$2차원이 된다.
  - 커널을 여러개 만들면 결과를 3차원으로 출력할수있다.


### Convolution의 역전파
- 커널이 모든 입력데이터에 공통으로 적용
- 역전파를 계산할때도 convolution연산이 나온다.


- 예시
  - x1,x2,x3,x4,x5가 입력
  - w1,w2,w3가 커널
  - o1,o2,o3로 출력
- 각각 loss $$\delta1,\delta2,\delta3$$를 얻는다.
- 역전파
  - x3에는 $$\delta_1w3+\delta_2w2+\delta_3w1$$
  - x3가 o1에는 w3를 통해 계산이 되고, o2에는 w2를 통해 계산되고, o3는 w1를 통해 계산이 되므로
  - 커널로 넘어간다.
  - 커널w1,w2,w3에는 $$(\delta_3x_3,\delta_2x2,\delta_1x3)$$그라디언트로 절단된다.
  - 커널에는 입력값x3*delta가 넘어간다.
  - 그라디언트들은 다른 입력들에 대해 적용된다.

-------

## optimization
- 최적화관련 용어
- gradient descent기법
- 용어 generalization,overfitting, cross-validataion
- gradient descent
  - SGD를 넘어서 최적화(학습)가 잘될 수 있도록 하는 다양한 기법 확인
- 최적화에대한 용어를 제대로 정의해야 한다.
  - 통일하여 정의하고, 컨셉을 잡아야한다.

### gradient descent
- 기울기가 줄어드는 방향으로 최솟값을 찾는다.
- 일차 미분을 사용, local minimum으로 수렴한다.


### Generalization
- 일반화성능
- 학습을 하면서, 학습데이터에 대한 에러는 줄어든다.
  - training error
- 어느정도 시간이 지나면 test error(사용하지 않은 데이터)가 늘어난다.
- 일반화는 training error와 test error의 차이

### overfitting
- training에서는 성능이 좋고, test에서는 성능이 낮을때

### cross-validataion
- train,test를 나눠서 학습과 검증
- 얼마큼으로 나누는게 좋을지
- k-flod validataion이라고 불리기도함
  - k개 나눔
- train데이터를 k-1 만큼 train, 1개는 validation한다.
  - 돌아가면서 validation을 한다.
- hyperparameter라는게 존재
  - 직접정하는거 - learning rate등
- cross-validation 으로 hyperparameter를 정한다.


### bias, variance
- variance 분산
  - 얼마나 퍼져있는가
  - 비슷한 입력되었을때 일관적인가
  - 높으면 overfitting
- bias 편향
  - 평균적으로 보았을때 target에 가까운지
  - 높으면 underfitting
- bias and variance tradeoff
  - 학습데이터에 노이즈가있을때(true target에 노이즈)
  - cost = bias^2 + variance + noise
  - bias를 많이 줄이면 variance가 높아지고, variance를 줄이면 bias가 높아진다고 한다.

### bootstrapping
- 학습이 100개 있으면 다 쓰는게 아니라 이중에 몇개만 활용하겠다.
- 학습이 고정되었을때 샘플링해서 여러 모델을 만들어서 진행하는것


### bagging boosting
- bagging(bootstrapping aggregating)
  -  학습데이터가 고정일때, 모델 하나를 만드는게 아니라
     - 여러개의 샘플리으로 모델을 여러개 만든다.
  - 여기서 나온 결과를 이용하겠다
    - 앙상블이라고도함
  - 여러 모델의 결과를 평균
  - 앙상블
- boostring
  - 학습데이터를 샘플링하여 모델을 만든다.
  - 이 모델들을 시퀀셜하게 이어서 사용
  - weak learner를 이어서 strong learner를 만드는거

### Gradient descent method
- stochstic gradient descent
  - 하나의 샘플을 통해서 gradient를 구하는거
- mini-batch gradient descent
  - 데이터의 일부를 사용하는것
- batch gradient descent 
  - 모든데이터를 사용하여 평균을 사용한다.

### batch-size matter
- 적절한 batch크기를 정해야한다.
- batch크기가 너무크면 sharp minimizers, 작으면 flat minimizers가된다.
  - batch가 작으면 좀더 좋다는 결론을 말한다.


-------


## gradient descent Methods
- 대부분 구현되어있어 사용하면된다.

### gradient descent
- wt+1 = wt - learning rate*gradient
- learnign rate가 중요하다.
  - 너무커도, 작아도 문제가 생긴다.

### momentum
- 관성
- 한번 흘러간 gradient를 어느정도 유지해준다.

### nesterov accelerated gradint
- lookahead gradient
- 다음위치로 이동하여 gradient를 보고 이동한다.
- momentum 진자운동같이 왔다 갔다 거리는것을 방지해준다고 한다.

### adagrad
- adapts
- 지금까지 얼마나 변화했는지 본다.
- G라고 하는 gradient가 얼마나 변했는지 저장해온것
  - 제곱되며, 역순으로 되어있어 변한것은 낮아지고, 아직 변하지 않은것은 높아진다.
- 너무 커져서 학습이 안될때가 있다.

### adadelta
- 너무 커진것을 방지
- 윈도우를 가진다.
  - 파라매터가 너무 크면 용량이 많아진다.
- learning rate가 필요없다.


### RMSprop
- stepsize가 추가됨
  - 러닝레이트랑 비슷한거인듯
- adagrad랑 adadelta합친거로 보임


### Adam
- adaptive moment estimation
- 위에서 나온 이론들을 섞은것
- 엡실론은 0으로 나뉘는거 방지하기용

-------


## regularization
- generalize를 잘하게 하고 싶은것
- 규제를 건다.
  - 학습에 반대, 방해하는것
- 학습데이터에만 잘되는게 아니라, 테스트에도 잘되도록 하는것

### ealry stopping
- 일정시간에 학습을 정지
- train,validataion을 나눠서 validation 에러가 늘어날때쯤 학습 종료

### parameter norm penalty
- 파라메터가 너무 커지지 않게
- loss를 줄이는건데 더하기를 해준다.
  - 웨이트를 줄이려한다.
- underfit방향으로 변형?


### data augmentation
- 데이터가 많으면 성능이 좋다.
- 데이터가 적으면, deep learning 이 성능이 더 않좋아진다.
- 데이터가 한정적이다.
- 주어진 데이터를 지지고 볶아서 늘리는거
  - 사진을 돌리고, 반전시키고, 찌그러트리는 변환을 적용
  - label이 변하지 않은 한도내에서 변형
    - 6을 뒤집으면 9가되는 경우는 피해야한다.

### noise robustness
- 입력에 노이즈를 적용
- weight에서 노이즈를 중간에 집어넣어준다.
- 성능이 오르는 경우가 있다.

### label smoothing
- 학습데이터를 두개 뽑아서 섞는다.
- mix-up
  - 두개의 이미지를 섞고, 라벨도 섞는다.
  - 고양이,개가 섞이고, 고양이0.5, 개0.5로 준다.
- cut-out
  - 이미지 일부를 자른다.
- cut-mix
  - 잘라서 붙인다. 개0.6, 고양이0.4로 label을 정해준다.

### drop out
- 랜덤하게 weith를 0으로 해준다.


### batch normalization
- 논란이 많다고한다.
  - internal covariate shift
- 내가 적용하려는 레이어를 스테틱스틱을 정규화?
- 간단한 분류하는데 있어 성능이 오른다고 알려져있다.







----


## 좀더 찾아보기
- cnn수식
- 국소적
- Radam
- adamP
- cross-validataion
  - 하이퍼파라미터는?
- bias, variance trade off?
  - 노이즈가 없으면 trade off가 없다?
- parameter panelty
  - 왜 커야 하는지, smooth해야 하는지
- batch normalization
- group normalization논문이 좋다고한다.



-----

## 피어세션 정리
- pytorch에서 shuffle의 의미


### further questions
- 올바르게 cross-validation을 하기위한 방법은?
- time series의 경우 k-fold cv를 사용해도 될까?
  - 안된다.
  - 영상의 행동예측에 사용되기도한다.
  - 주가예측에서도 사용한다.


----

## 과제
- opt빈칸채우기
  - regression with different optimizers
  - 다양한 최적화와 regression(선형회귀)
  - optimizer에 따라 달라지는 성능
- momentum이 sgd보다 좋은이유
  - sgd는 그 batch만 표현하는 모델만든다 = iter이 더 많아야 한다.
  - momentum은 이전의 gradient도 사용하니좀더 시야를 넓히는 효과가 나온다.