---
layout: post
title: 16.NLP소개와 bag-of-words, Word Embedding
categories: AI boost
tags: [ai,boost,NLP,Bag-of-words,naivebayes classifier,Word2Vec,GloVe]
toc : true
math : true
---


# 강의 복습 내용
- 단어를 벡터로 표현하는 방법, 문서를 벡터로 표현하는 방법
- 전체적인 NLP 트렌드
- NLP의 word2vec과 glove를 알아보기

## 얻은 지식
- bag-of-words
- word Embedding    
  - word2vec
  - GloVe


-----

## NLP소개
- natural language processing
- NLU
  - 문장을 이해
- NLG
  - 문장을 생성
- 자연어 처리 관련 기술을 학습
- 주어진 문장을 보고, 다음 단어를 예측하는 language modeling
- 기계번역 등


### natural language processing 분야
- 학회
  - ACL,EMNLP,NAACL
- low-level
- word and phrase level
- sentence level
- multi-sentence and paragraph level(문단)
  - entailment prediction
  - question answering
  - dialog systems 
  - summarization 요약



### text mining 분야
- 빅데이터와 관련되어있다.
- 과거의 모든 기사들로부터 특정 키워드에 따라 분류하고 분석하고 트렌드를 분석할때
- 서로다른 키워드도 비슷한 의미일때 grouping이 필요
- 사회과학(sns)와 많이 관련되어있다.


### information retrieval 분야
- 검색 기술을 연구
- 검색 기술은 어느정도 성숙하여 다른 분야에 비해 성장이 느리다고 한다.
- 추천 시스템 기술을 연구



### 자연어 처리의 발전과정
- 인공지능과 딥러닝이 활발이 적용
- 단어단위로 분리하고, vector로 표현하는 과정이 필요
- RNN모델을 사용하였으나, 성능이 더 좋은 transformer모델을 사용
- 기계번역시 언어의 규칙을 통해 번역하기 보단, 딥러닝을 통해 단어단위로 학습을 진행한게 더 성능이 좋았다고 한다.
- 자가지도 학습(self-supervised training)으로 일부를 지우는 데이터를 학습하므로 문장을 이해하는 학습방식이 있다.
  - gpt-3
- 자가지도 학습은 많은 데이터와, 리소스가 필요하다.


## Bag-of-Words
- 딥러닝 이전에 많이 활용된 단어와 문장을 숫자로 표현하는 방법



### 예시
- 문장으로 부터 단어단위로 분리하여, 사전(vocabulary)으로 데이터를 저장한다

- "john really really loves this movie"
  - {john,really,loves,this,movie}
- 이 vacabulary를 one-hot vector형식으로 표현한다.
  - 각 단어마다 거리는 루트2, 유사도는 0으로 나타난다.
  - 단어의 의미에 상관없이 모두 동일한 관계를 가지고 있다.
- 하나의 문장에 나오는 단어들의 one-hot vector들을 더한값으로 표현이 가능하다.
  - 이를 bag-of-words vector라고 한다.
  - 가방에 각 단어의 one-hot vector를 넣는다고 생각



### NaiveBayes Classifier
- 문서가 분류되는 카테고리가 C개
  - 정치,과학,연예 등
- 특정한 문서 d



$$C_{MAP} = \underset{c\in C}{argmax} P(c \mid d) $$ 
- C에서 하나의c에 대해 가장 높은 확률을 찾는것이 목적
  - MAP이 그런 의미, maximum a posteriori = most likely class

$$\underset{c\in C}{argmax} \frac{P(d \mid c)P(c)}{P(d)}  $$ 
- bayes 규칙을 통해 식을 표현
- 분모 P(d)는 d는 하나의 고정된 문서이므로 상수로 본다.
  - 이는 무시가 가능하다고 한다.


$$C_{MAP} = \underset{c\in C}{argmax} P(d \mid c)P(c) $$ 
- P(d)가 무시되어 이렇게 표현된다.

$$P(d \mid c)P(c) = P(w_1,w_2,\cdots ,w_n \mid c)P(c) = P(c)\prod\nolimits_{w_i\in W} P(w_i\mid c)  $$ 
- P(d|c)는 c가 고정되어있을때, d가 나타날 확률을 표현
- d문장은 각 단어 w1부터 wn까지의 단어가 동시 사건으로 볼 수 있다.
- c가 고정되었을때 각 단어가 등장할 확률이 독립이라 할때, 곱으로 표현을 할 수있다.



## word Embedding
- word2Vec, GloVe
- 단어를 특정한 차원의 vector로 바꾸는것
  - 특정한 좌표상에 단어를 하나의 점으로 표현하는것
- 의미상의 유사도를 각 단어마다의 거리로 표현된다.
  - cat과 kitty는 비슷, hamburger와 cat은 멀다.
- 예시로 긍정표현들은 비슷한 vector표현을 가질것이다.2


### Word2Vec
- word embedding을 학습하는 방법
- 유사한 문맥을 가진 문장의 단어들은 비슷한 의미를 가질것이다는 가정으로 시작
  - 고양이가 갸르릉 거린다.(the cat purrs)
  - 고양이가 쥐를 사냥한다 (this cat hunts mice)
  - cat이라는 단어를 기준으로 purrs,hunts,mice,the,this가 관련있다고 생각 한다.
- 하나의 단어는 주변의 단어들을 통해 의미를 알 수 있다는 아이디어
  - 하나의 단어를 숨기고 주변 단어들을 통해 예측하는 방식으로 학습


### word2vec의 학습
- 주어진 문장이 "i study math"
- 학습데이터를 word별로 분류하는 tokenize를 진행
  - {i,study,math} vacabulary로 표현
  - 각 단어는 사전의 크기의 one-hot vector를 가진다.
  - i=[1,0,0], study=[0,1,0] math = [0,0,1]
- sliding window를 통해 중심단어와 앞뒤의 단어를 각각 출력으로 가지는 쌍을 구성한다.
  - window가 3이면 바로 앞과 뒤만 본다.
  - (i,study) i 일때, 바로 뒤의 study
  - (study,i) study일때, 바로 앞의 i
  - (study,math) study 일때, 바로 뒤의 math
  - ...


- 학습을 위해 2layer의 네트워크를 구성
  - $$ y = softmax(W_2W_1x)$$로 계산된다.
    - 계산이 좀 특이하게 뒤에서부터 계산된다.
  - 입력과 출력은 vacabulary의 크기가 된다.
  - 히든레이어의 노드수는 사용자의 hyper parameter로 정해진다.
    - word embedding의 좌표공간의 차원수로 정해진다.
    - 여기서는 2차원으로 설정
  - W2는 (3,2) W1은 (2,3)x는 (3,1)이 된다.
- 입력(3,1)에서 히든레이어는 W1x가 계산되어 (2,3)(3,1)로 (2,1)을 가진다.
- 출력에는 W2 W1x이므로 (3,2)(2,1)로 (3,1)이 된다.
  - 나온 결과는 softmax를 한다.


![이미지](https://user-images.githubusercontent.com/24247768/107909845-79901880-6f9c-11eb-9c29-576aebe60bb9.png)


- 초반 w1과 곱해질때 입력이 one-hot이므로 1로된 부분만 추출하는 형식이 된다.
  - 이를 embedding 레이어라고 한다.
  - 이 과정에서는 행렬곱이 아닌 one-hot의 1이되는 index의 값만 뽑아오는 방식으로 진행한다.
- 이후 w2와 행렬곱이후 결과의 softmax가 ground truth에 맞게 학습을 위해서는 정답을 +무한대, 아닌부분을 -무한대로 설정하여 학습을 한다.
  - w1과 w2에서 정답에 위치한 부분은 높게, 아닌부분은 낮도록 조정하면서 학습한다.

- 특징
  - 백터 연산으로 의미적으로 연관관계를 알 수 있다.
  - 여-남 = 여왕-왕



### GloVe
- global vectors for word representation
- 언어마다의 관계를 미리 계산하여 학습한다고 한다.
- 윈도우에서 동시등장확률을 미리 계산
  - w2v는 하나씩 보면서 학습하게 되어 전체적인 부분을 못본다고 한다.
- 빠르다.
- 관계
  - 여자,남자와 성별이 반대라는 개념을 학습하여 brother,sister의 관계도 파악
  - 우편번호, 도시 와같은 관계
  - slow, slower, slowest와 같은 형용사의 관계도 학습한다.



----



## 좀더 찾아보기
- Bag-of-Words
- one-hot-encoding
- Naive Bayes Classifier
- word embedding
- word2Vec
- GloVe



-----



## 피어세션 정리
- 다들 쉬었다. 
  - 어느정도 공부하심
- `오토인코더의 모든것` 볼만하다.
- 질문
  - word2vec의 차원이 왜 2개인지
  - class 와 label
- 두가지 방식 tensor부분이 어렵다.
- 역전파란?
  - [참고영상](https://www.youtube.com/watch?v=d14TUNcbn1k)
  - 따라해보면 좋다.
- loss와 cost, objective function 차이
  - [참고](http://blog.naver.com/PostView.nhn?blogId=qbxlvnf11&logNo=221386278997&parentCategoryNo=&categoryNo=&viewDate=&isShowPopularPosts=false&from=postView)
  - cost는 loss의 평균
    - loss는 batch할때 마다, cost는 전체



### further question
- word2vec의 단점 
  - 윈도우의 크기만큼의 관계를 보기때문에 전체적인 정보가 반영되기 어렵다.
- glove
  - w2v는 하나씩 다해야한다.
  - glove는 문장 전체에 대해 윈도우크기에서 동시 등장을 계산 다 한다.




----



## 연휴후기
- 요새 의욕이 많이 떨어지고 무기력하고 집중력이 많이 떨어졌다.
- 연휴동안 뭔가 하고싶었는데, 다 안하고 그냥 쉬었다.
- 먼저 주어진 과제부터 하나씩 해결하면서 진행하면서 의욕을 높여야겠다.

