---
layout: post
title: 17.RNN과 LSTM,GRU
categories: AI boost
tags: [ai,boost,RNN,LSTM,GRU,language model]
toc : true
math : true
---


# 강의 복습 내용
- RNN을 활용하는 방법과 language model
- 순서를 가진 문장을 표현하는데 사용되는 RNN
- language model은 이전에 등장한 단어를 조건(condition)으로 다음에 등장할 단어를 예측하는 모델
- RNN을 이용한 character-level의 language model


## 얻은 지식

-----

## RNN
- Recurrent neural networks


![unrolled](https://user-images.githubusercontent.com/24247768/108052065-f6032400-708e-11eb-9878-d5e302ee0db2.png)


- A모듈이 반복적으로 사용된다.
- 이렇게 풀어서 표현한것을 unrolled라 한다.

![rolled](https://user-images.githubusercontent.com/24247768/108052117-074c3080-708f-11eb-86d8-84488baf5272.png)


- 특정 time steps에서의 하나의 vector를 예측하려 한다.
  - 그림은 rolled형태



- 구성요소
  - $$h_{t-1}$$ old hidden-state vector(이전 RNN에서 계산된것)
  - $$x_{t}$$ input vector at some time step(입력)
  - $$h_{t}$$ new hidden-state vector(현재 계산된것)
  - $$f_w$$ RNN function with parameters W
    - $$h_t = f_w(h_{t-1},x_t)$$ 같은 함수,W가 매번 사용되는 특징이 있다. 
  - $$y_{t}$$ output vector at time step t(예측값)
    - 매타임마다 계산 또는 마지막에만 계산될 수 있다.

- hidden에서 계산되는 방식

![강의스크린샷1](https://user-images.githubusercontent.com/24247768/108048837-d964ed00-708a-11eb-9913-dec118cc10e4.png)


$$h_t = tanh(W_{hh}h_{t-1} + W_{xh}x_t) $$
- xt가 3차원, ht-1이 2차원이라 할때 5차원을 생각할수있다.
- 이때 ht를 2차원을 가지므로 계산되는 W는 2차원의 5축을 가진 vector를 생각할 수 있다.
- 이를 행렬곱으로 계산할때, W의 3개의 축은 xt를 담당하고, 나머지2개의 축은 ht-1을 담당하여 계산이 된다.
  - xt를 담당하는 W를 Wxh, ht-1부분은 Whh라고 표시한다.
- 이를 계산식으로 하면 ht = Wxh xt + Whh ht-1로 표현된다
- Wxh는 W{xt->ht}, Whh는 W{ht-1 -> ht}라고 알 수 있다.

$$ y_t = W_{hy}h_t$$
- 위 식도 h_t를 계산했듯이 생각하면
- W_hy는 W{ht->yt}를 생각 할 수 있다.



### type of RNN
- one-to-one
  - 입력과 출력이 단 하나
  - sequence, time step데이타도 아닌것
- one-to-many
  - 입력하나로 여러 출력
  - 이미지를 입력, 이미지에 대한 설명을 출력하는 image Captioning
  - 입력은 첫번째 timestep에서만 들어간다. 그외에는 0으로된 의미없는 값을 넣는다고 한다.
- many-to-one
  - 입력은 timestep마다, 출력은 마지막 time step에서만
  - sentiment classification으로 긍정/부정을 예측할때
- sequence-to-sequence(many-to-many)
  - machine translation(번역)
  - 입력/출력이 분리되어 입력이 다 되고나서 처리한다.
- many-to-many
  - 입력이 끝날때까지 기다리는 딜리에가 없이 바로 출력이 진행된다.
  - video classification on frame level
  - 비디오영상을 입력할때 해당 씬마다 어떤 행동을 하는지 분류할때 사용

## character-leve language model
- 다음글자가 뭐가 나올지 예측하는것
  - word(단어),character(글자) 단위로 생각이 가능하다.
- hello를 예시하면 [h,e,l,o]라는 vocabulary를 생성
  - 각 character를 one-hot vector로 만든다.
- 학습하고나서면, h가 입력되었을때 e를 예측, e가 입력되면 l을 예측하는 방식으로 진행될 것이다.



![강의스크린샷2](https://user-images.githubusercontent.com/24247768/108048995-103b0300-708b-11eb-92de-9008d911ea1a.png)


$$ht = tanh(W_{hh}h_{t-1} + W_{xh}x_t)+b$$

$$ logit = W_{hy}h_t + b $$
- 결과는 vacabulary와 같은 크기로 출력
- softmax로 확률이 나오게된다.
- 예측값과 ground truth를 맞추도록 학습이 진행


- 테스트 할때 입력후 예측된것을 다음 입력으로 사용한다.
  - 이를 통해 무한하게 특정단어를 만들어 낼 수 있다.
  - 이를 응용하면 주식을 예측할 때 사용이 가능하다.

- 좀더 확장하면 하나의 세익스피어의 희곡같은 긴 문장도 학습이 가능하다.
  - 특정 등장인물 대본, 논문, 프로그래밍 코드 등 

### BPTT
- back propagation through time
  

![강의스크린샷3](https://user-images.githubusercontent.com/24247768/108051896-c6541c00-708e-11eb-968c-5d5f6dcf33b1.png)

- 입력, Wxh,Whh로 ht가 되고, Why로 출력이 나온다.
  - 여기서 나온 loss들을 역전파로 계산해야 하지만, 문장이 길어지면 loss계산이 커지고 반영도 잘 안된다.
  - 특정 단위로 잘라서 역전파를 진행하게 된다.

### 학습방식

- 어떻게 학습을 하고, 어떤식으로 rnn에 저장되는지를 분석하는 방법은?
  - RNN의 ht라는 hidden state vector에 정보들이 저장된다.
  - 이를 역추적 하기위해 hidden state vector 일부값을 고정하여 어떤식으로 변화되는지를 보면서 분석이 가능하다.

![강의스크린샷4](https://user-images.githubusercontent.com/24247768/108049337-79227b00-708b-11eb-9e34-20abe4f30df4.png)



- 위의 분석에서의 예시로 큰따음표가 열렸다, 닫혔다를 rnn의 특정 값(cell)이 파악하고 있다는것을 알 수 있다.


### RNN의 문제
- vanishing/exploding gradient 문제
- gradient가 너무작아지거나, 너무 커지는 계산이 된다.
  - 여러번의 rnn계산을 하나로 표현하게 될때 이를 역전파 계산을 위해서는 편미분을 계산해야한다.
  - whh라는 값이 timestep만큼 거듭제곱되어 증폭된다.


## LSTM,GRU
- Long short-term memory
- Gated Recurrent Unit

- 바닐라 RNN의 단점 해결

### LSTM
- gradient vanishing을 해결
- 단기기억을 time step을 지나가는데 기억하게 하도록 하는 아이디어



- 이전에 입력이 Ct,ht로 두개가 들어온다.
  - c는 cellstate라고 한다.

$${C_t,h_t} = LSTM(X_t,C_{t-1},h_{t-1}) $$

- 핵심은 cell state이다.

### LSTM 계산

![lstm](https://user-images.githubusercontent.com/24247768/108057506-171b4300-7096-11eb-9607-a8ac8be2714c.png)

- 좌측부터 f,i,g,o이다.
- i(input gate) 입력 - 입력을 사용 할지 말지
- f(forget gate) 잊기 - 지울지 말지
- o(output gate) 출력 - 얼만큼을 출력으로 할지
- g(gate gate) 입력 - 얼만큼을 입력을 사용할지
  - 입력 예비군
  - tilde라는 위에 ~표시를 한다.

- cell state는 기억해야할 모든정보를 가진다.
- hidden state는 cell state의 모든 정보로부터 필요한 정보를 필터한정도로 생각


## GRU
- lstm을 개량


![GRU](https://user-images.githubusercontent.com/24247768/108057683-49c53b80-7096-11eb-86e3-76a5d5c4edf0.png)

- cell state vector가 사라졌다.
  - hidden state vector로 통합되었다.
- lstm은 f와i라는 두개의 게이트를 통했다면 gru는 z와 1-z로 하나의 게이트로 경량화 계산했다.


- ht라는 출력에 forget되는 z를 통해 나온것을 이용
  - 입력xt와 이전 ht-1을 이용하여 forget gate 인 z를 통과
  - 입력 xt와 입력게이트r 이전 ht-1을 tanh를통하여 $$ \tilde{h_t}$$를 계산
  - z를 이용하여 $$ \tilde{h_t}$$에는 zt를 ht-1에는 1-zt를 곱하는 방식으로 가중되는 방식으로 계산한다.

### 역전파
- forget gate를 곱해가고, 덧셈으로 cell state에 업데이트 되기때문에
- 덧셈은 역전파일때 gradient를 복사해주는 연산이된다.
  - 편미분이니까
- 멀리있어도 큰 변형없이도 넘어가게된다.


### 요약
- RNN 유연한 모델
- vanila RNN은 간단하지만, 학습할때 explode 또는 vanish가 발생
- LSTM이나 GRU를 사용
  - cell state, hidden state를 사용하며 덧셈을 이용하기때문에 RNN의 단점 해결



----

## 좀더 찾아보기
- lstm의 cell state

-----


## 피어세션 정리
- 마스터클래스 질문 2개 만들기
  - nlp를 공부하는데 있어 언어학에대한 공부가 추가적으로 필요할지
    - 관련된 도서나 다큐먼트 추천
    - 교수님 입장에서는?
  - nlp가 다른 딥러닝 분야랑 차별화된 점?


### further question
- bptt이외에 rnn/lstm/gru의 구조를 유지하면서 gradient vanishing/exploding 문제를 완화하는 방법은?
  - [참고1](https://wikidocs.net/61375)
- rnn/lstm/gru 기반의 language model에서 초반 time step의 정보를 전달하기 어려운 점을 완화하는 방법은?
  - (문제 내용을 학습이 초반의 내용이 끝까지 영향이 안간다고 생각)resnet의 skip connection을 사용해본다면?

