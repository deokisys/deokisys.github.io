---
layout: post
title: 18.sequence to sequence와 beam search
categories: AI boost
tags: [ai,boost,NLP, squence to sequence,beam search,precision , recall,K-measure,BLUE, BLUE score]
toc : true
math : true
---


# 강의 복습 내용
- sequence를 encoding, decoding할 수 있는 `sequence to sequence`
  - rnn과 `attention`을 결합한 모델
  - rnn의 단점을 보완하고자 attention(alignment)기법이 처음 등장
  - attantion의 종류와 translation task는?

- 문장을 decoding하는 대표적인 알고리즘 `beam search`
  - 문장을 generation할때 모든 경우를 보는 것은 비효율적이다.
  - 이러한 문제의 대안으로 제안된 Beam Search
- 번역된 문장을 평가하는 대표적인 metric인 `BLUE score`
  - 모델을 평가하기위한 점수


## 얻은 지식

-----

## Seq2Seq
- rnn의 구조에서 `many to many` 형태이다.

![enoder decoder 그림](https://user-images.githubusercontent.com/24247768/108192658-e26fc000-7157-11eb-92be-f07b64c893dc.png)


- 입력을 받는부분을 encoder, 출력을 만드는 decoder
- 내부 모듈은 lstm을 사용하였다.
- 모든 encoder를 받고나서 thought vector라는 하나의 hidden state를 통해 정보를 넘기고, 이를 decoder가 출력해낸다.
- decoder결과는 start(SOS토큰)부터 end(EOS토큰)이 나올때까지 반복적으로 수행하면서 출력한다.


### Seq2Seq model with attention
- 시퀀스입력이 매 time step마다 입력받는다.
  - 하지만, encoder는 입력에 대한 압축된 정보를 decoder에게 넘기면서 문제가 생긴다.
    - 초반에 나온 단어의 정보가 소실되거나 변질되는 문제가 있다.
    - bottleneck문제
    - 이를 위해 attention을 사용
- attention모듈
  - 기존 encoder는 입력을 다받고 나서 decoder를 수행
  - attention은 encoder에서 매 입력받으면서 나오는 hidden state vector를 전부 활용하는 방식으로 decoder가 수행한다고 한다.


![attention 그림](https://user-images.githubusercontent.com/24247768/108192556-c1a76a80-7157-11eb-9106-2bbaf01c02ab.png)

- 매 입력마다 encoder는 hidden state vector를 계산
- deocoder의 hidden state vector는 encoder에서 나온 hidden state vector와 내적한다.
  - 이를 통해 attention scores라는 decoder의 각 입력들마다의 내적에 기반한 유사도로 볼 수 있다.
  - 이를 softmax로 확률값을 구할 수 있다. 이를 각 입력마다의 가중치로 볼 수 있다.
  - 이 가중치를 통해 출력할때, 각 입력들에서 어느 입력 정보를 집중(attention)할지 알 수 있다.
- attention 모듈
  - 입력은 decoder hidden state vector하나, encoder hidden state vector의 세트
  - 출력은 encoder hidden state vector의 가중 평균
- 이를 통해 출력은 attention 결과를 이용해 decoder의 예측값을 출력한다.

![attention 학습 그림](https://user-images.githubusercontent.com/24247768/108192977-41353980-7158-11eb-8e37-0a89a0574702.png)


- decoder의 역할
  - 두가지 역할을 학습 하도록 해야한다.
    - 1. attention을 계산할때 
    - 2. attention과 결합하여 예측
  - 학습할때는 ground truth를 입력으로 사용 한다.
    - teacher forcing이라 한다.
    - 실제 상황과 약간 괴리가 느낀다고 한다.
    - teacher forcing과 예측값을 사용하는 방식을 섞는다고도 한다.


### attention mechnisms
- attention을 단순 내적이 아닌 다른 방식의 attention방식


- $$ score(h_t,\bar{h_s}) = \cases{h_t^T \bar{h_s} & dot \\ h_t^T W_a \bar{h_s} & general \\ v_a^T tanh([h_t;\bar{h_s}]) & dot } $$
  - ht는 decoder의 hidden state vector, hs는 encoder의 특정 word의 hidden state vector
  - dot은 내적(행렬곱)
  - general은 중간에 학습가능한 행렬을 추가
  - concat 2개의 layer의 네트워크
    - 두 벡터를 concat, 이를 첫번째 가중치 Wa계산으로 첫번째 layer를 거친다.
    - tanh 을 통해 non-linear값을 얻는다.
    - va는 두번째 layer가중치로, 통과하여 하나의 scala값을 얻는다.


### attention 의 장점
- NMT(기계번역)의 성능이 올랐다.
  - decoder가 sorce의 어느부분을 집중할지가 유용했다.
- bottleneck을 해결
- gradient vanishing도 해결
- interpretablility(해석력)을 제공해준다.
  - decoder가 어느부분을 attention했는지를 확인할 수 있다.

-----

## beam search
- 자연어 생성모델에서 test에서 좀더 성능이 좋도록 한다.

### greedy decoding
- 매번 현재 time step에서 가장 좋아보이는 단어를 하나 골라서 진행하는 방식
  - 가까운것만 봐서 greedy 하다.
- 잘못 할당했을때 undo(실행취소)가 불가능하다.
  - 이를 어떻게 고칠까

### Exhaustive search

$$ 
\begin{align}
P(y \mid x) & = P(y_1\mid x)P(y_2 \mid y_1, x) P(y_3\mid y_2, y_1, x)\cdots P (y_T \mid y_1,\cdots , y_{T−1}, x) \\ & = \prod_1^T{P(y_T \mid y_1,\cdots , y_{T−1}, x)}
\end{align}
$$

- 입력이 x일때 y1을 출력, x와 y1을 가지고 y2를 출력, 이런 방식을 연속적으로 계산하는 방식
- y1을 제일 큰 확률값이라고 해도 그 뒤쪽 결과가 낮을 경우가 있음
  - 전체적인 확률값을 보고 결정해야한다.
- 모든 y의 확률을 계산해야한다.
  - 너무많은 계산량이 필요하다.

### Beam search
- greedy와 exhasutive의 중간정도
- 핵심
  - 매 step마다, 가장높은 k개의 가능한 가짓수를 계산
  - 이를 hypothesis(가설)이라 한다.
  - k는 beam size라고 한다(5-10정도의 크기)
- log를 취한 점수를 사용
  - 덧셈으로 표현
- globally한 결과를 가져오진 않는다.
- 모든 경우를 보는것 보다는 효율적이다.

### Beam search 예
- beam size = 2


![시작사진](https://user-images.githubusercontent.com/24247768/108194746-51e6af00-715a-11eb-8ef6-2a15cc68cab5.png)

- 매번 beam size만큼의 제일큰 확률을 구한다.
- 그중 제일큰 beam size들을 구한다.

### Beeam search의 종료
- greedy 는 END토큰이 나오면 종료이다.
- Beam Search는 각자 END가 나오게 된다.
  - END토큰이 나오면 임시공간에 저장하는 과정을 반복한다.
- 종료는 언제?
  - 1 T라는 최대 time step만큼만 진행
  - 2 임시저장공간의 갯수가 n만큼 되었을때

### Beam search 결과
- 저장공간의 가설들을 계산하여 최대인 것을 결과로 사용한다.
  - 문제점: 가설이 길어질수록 점수가 낮아진다.
- nomalize를 진행한다.
  - 단어의 갯수를 나눠서 평균을 낸 결과로 판단한다.


## BLUE score
- 생성모델의 품질과 정확도를 측정하는 척도
- 단순하게 두문장을 한 단어씩 비교해서는 제대로된 정확도가 계산이 안된다.

### precision and recall
- reference(원본) 
- predicted(예측)

$$ precision = \frac{\sharp (correct\, words)}{length \, of\, prediction}$$

- precision은 정밀도
  - 위치상관없이 정답과 일치한 단어 갯수 / prediction 단어 갯수
  - 예측된 결과를 가지고 실질적으로 느끼는 정확도
  - 구글이나 네이버같은 검색할때 나오는 결과의 정확도로도 사용된다.

$$ recall = \frac{\sharp(correct\, words)}{length\, of \,reference}$$

- recall은 재현율
  - 위치상관없이 정답과 일치한 단어 갯수 / reference 단어 갯수
  - 검색에서 키워드로 검색했을때, 원하는 결과의 갯수에서 얼마나 사용자에게 노출을 했는가
    - 헷갈린다.
  - 모든 정답에서 얼만큼을 나타냈는가를 보여준다.

- 위의 두가지 결과값을 결합하여 하나의 결과를 도출하기
  - 산술평균값 - 둘값을 더하기/갯수 
  - 기하평균값 - 둘값을 곱한것을 루트
  - 조화평균값 - 두값의 역수의 산술평균을 역수
  - F-measure를 이용해 평균을 구한다.
    - 조화평균이라고 한다.

$$ F-measure = \frac{precision \times recall}{\frac{1}{2}(precision + recall)}$$ 

- 단점
  - 순서가 다른 문장에 대해서는 따로 패널티가 없다.
  - 정답 문장의 순서를 꼬기만 해도 100%로 계산이된다.

### BLUE score
- BiLingual Evaluation Understudy
- 개별적인 단어마다의 계산도 가능
- n-gram n개의 연속된 단어를 이용한다.
- 1개부터 4개까지의 n-gram에 대한 precision만 계산을 한다.
- brevity penalty를 추가하였다.
  - reference보다 작은단어를 만들었을때 그 비율만큼 precision값을 낮춘다는 의미가된다.


$$ BLEU = min(1, \frac{length \, of \, prediction}{length \, of \, reference})(\prod_{i=1}^4{precision_i})^{1/4}$$



----

## 좀더 찾아보기
- teacher forcing


-----

## 마스터 클래스
- 주재걸 교수님
- 현재 Vision을 하고계심
  - 자기가 흥미있는 도메인부터 진행
- 반어법
  - 해당 연구는 아직 지켜보는 단계라고 한다.
- nlp가 유리하지 않을까
- 프론트엔드
  - chai
  - ui도 연결된 논문도 있다고 한다.
- 자신만의 경쟁력을 만들자!


-----



## 피어세션 정리
- 과제 질문

### further question
- BLEU의 문장평가의 단점은?

-----

## 과제해설
- preprocess
  - target에는 sos,eos를 넣어줘야한다.
    - -2만큼 줄어들어서 데이터 유실이 일어날 수 있다고 한다.
    - 서로 다른 모듈이므로 꼭 맞추지 않아도 된다.
- bucket
  - 랜덤하게 batch로 뽑으면 pad토큰이 많이들어갈 수 있다.
  - 비슷한거로 모여서 학습을 정했다.
  - 예)서로 max_pad_length(5)만큼 차이나는 것들을 모아서 사용
  - index list를 리턴
  - bucketIterator를 참고해봐라
  - +1하는 이유는?
    - 6의 몫을 사용해야 5이하인것들을 구하기위해 값이 나온다.
    - 가장 짧은것을 기준으로 하기 위해 짧은것을빼줬다. 그러고 +1을 해줬다.
      - 가장짧은것은 0이되니까 그걸 1로 맞추려고 해줬다.
    - max_pad_leng+1은 +1하든 말든 같다고 한다.?
    - 단위로 따지고 몫을 사용했다는것을 집중해서 보면된다고 한다.
      - 연산상에 큰 차이는 없다고 한다.
- collate_fn
  - 데이터셋을 모델에 batch단위로 넘길때 batch를 구성을 변경하는 함수
  - 이미 batch단위로 샘플을 받은 상태
  - bucket이 완료된것을 받은것
