---
layout: post
title: 20.GPT-1,2,3와 BERT
categories: AI boost
tags: [ai,boost,NLP,GPT-1,GPT-2,GPT-3,BERT]
toc : true
math : true
---


# 강의 복습 내용
- GPT-1과 BERT
  - Transfer Learning, Self-supervised Learning, Trasformer를 사용
  - 기존 자연어처리 task를 압도하는 성능을 보여줌


## 얻은 지식

-----

## Self-Supervised Pre-Training Models
- GPT-1
- BERT


### 최근 트렌드
- Transformer의 self-attention block은 NLP이외의 범용적이게 사용되었다.
- Transformer를 좀더 깊게쌓아서 self-supervised learning 으로 대용량으로 학습하고 transfer learning을 통해 성능을 올렸다고 한다.
  - BERT,GPT3,XLNET,ALBERT,RoBERTa,Reformer,T5,ELECTRA
- self-attention은 빠르게 다른 분야에 확장되고있다.
  - recommender system, drug discobery, computer vision 등
- self-attetion기반에서 자연어 생성에서는 여전히 greedy decoding방식을 벗어나지 못하는 문제가 있다고 한다.

## GPT1
- 다양한 special token을 제안하여, simpe한 task부터 NLP의 task를 수행하는 통합된 모델

![왼쪽긂]
- transformer block을 16개 쌓음
- 모델을 통과하여 예측(prediction과 분류(classifier)를 학습


![가운데 그림]
- classfication은 extract를 긍정/부정으로 학습을 진행하여 분류를 가능하도록 함
  - query는 마지막 extract를 이용하여 예측하는 방식
- entailment는 두가지 문장을 비교할때 `delim`을 토큰을 사이에 넣어 하나의 문장으로 만들어 학습
- similarity나 multiple choice는 여러 학습된 결과를 결합하는 방식으로 학습


- transfer learning 활용
  - 다음단어 예측(predict)과 더불어 좀더 특별한 task를 수행하길 원할때 추가적으로 학습을 진행
  - 기존12개 레이어는 이미 학습이 완료된 상태
    - word별 인코딩 벡터를 활용
  - 이미 학습된 transformer인코더에 원하는 임무에 대한 학습을 위 새로운 레이어 하나를 추가
    - 원하는 임무를 위한 새로운 데이터를 사용
  - 기존 12개 레이어는 이미 학습하였으니 learning rate는 낮게 설정

- pre trainin한것으로 부터 유용한 지식을 얻는 요소는?
  - 어떤 목적이나 따로 label이 존재하지 않음
  - self-supervised learning이다.
    - 단순 문장의 다음을 예측하기만 한다.
  - 대규모 데이터로 학습한 정보를 가짐

## BERT
- pre-training of Deep Bidirectional Tranformers for Language Understanding
- gpt와 같이 language model로 문장의 다음단어를 예측하는 모델로 pretraining을 수행
- transformer이전에 lstm을 이용한 접근이 있었다.
  - ELMO

### masked Language Model
- 기존 language model은 오직 문장의 오른쪽또는 왼쪽을 사용한다.
  - 왼쪽과 오른쪽 둘다 보고 예측하면 좀더 유의미 하지 않을까
- BERT의 pretraining Task로 사용
- MLM(masked Language Model)
  - ![그림]
  - 일정 확률로 문장의 일부를 랜덤하게 mask로 치환하여 이를 맞추는 방식으로 학습이 진행
  - 15%의 단어를 mask로 치환
    - 너무 높으면 보이는 단어가 너무 적어짐, 너무 낮으면 학습이 충분하지 않음
- 문제점
  - mask token never seen during fine-tunning
- 해결
  - 15%의 mask를 전부다 학습마다 사용하지 않는다
    - 80%는 mask를 적용
    - 10%는 아예 다른 단어로 변환
    - 10%는 정답 단어를 적용

### Next Sentence Prediction
- 문장레벨의 task를 대응하기 위한 pretraining
- 학습
  - 이문장이 연속적으로 나오는 문장인지 아닌지를 binary classification을 수행
  - 입력
    - `[cls]the man went to [mask] store[sep] he bought a gallon [mask] milk[sep] `
    - 어떤 task인지 표현하는 토큰을 맨 앞에 넣어준다.
      - CLS는 다수의 문장관련 수행하는 토큰
    - 하나의 글에서 두개의 문장을 뽑아서 sep토큰을 사이에 넣어 하나의 문장으로 만든다.
  - 출력
    - `isNext` 또는 `NotNext`
    - 이 두 문장이 이어지는 문장인지 판단하여 출력

### summary
- 모델 구조
  - self attention block을 사용
  - bert base : L=12(block 갯수) H=768(self attention마다 인코딩벡터의 차원) A=12(head갯수)
  - bert large : L=24 H=1024 A=16
- 입력
  - WordPiece embeddings
    - 단어를 좀더 쪼개서 사용
    - pretrain -> pr, train같이
  - learned positional embedding
    - 학습하여 최적화된 position을 사용
  - CLS - classification embedding
  - SEP으로 문장을 분류
  - segment embedding
- pre training task
  - masked LM
  - Next Sentence Prediction

### input Representation

![그림]
- segment embedding이란
  - 입력은 하나의 문장이므로 position은 그에 맞게 표현
  - 하지만 sep단위로 문장이 여러개라는 정보가 필요하다.
    - 각 문장마다 독립적으로 position을 가질 수 있게 해준다.
  - 이를 위해 문장단위를 표현해주는게 segment embedding


### BERT와 GPT-2의 차이는

![그림]
- gpt는 다음단어에 대한 정보가 오지않는다.
  - 자기자신과 이전단어에 대한 정보만 사용
  - masked self attention
- bert는 masked까지 포함하여 모든 단어를 접근


### Fine-tuning process

![그림]
- 미리 학습한 BERT로 수행하는 다양한 task
  - 두문장 분류, 한문장, 질의, 문장에서 각 단어마다 tagging하는 task를 수행 가능하다.


### BERT와 GPT-1차이는
- training size
  - GPT는 800M, BERT는 2500M
- 스페셜 token  
  - BERT는 SEP,CLS,문장을 구분하는 embedding이 pre-training에 사용
- batch size
  - BERT는 128,000단어, GPT는 32000단어
- task-specific fine-tuning
  - GPT는 같은 learning rate 5e-5로 진행, BERT는 task-specific fine-tuning learning rate를 사용

### MRC,Qustion answering
- Machine Reading Comprehension
- 질의 응답의 형태
- 주어진 지문에서 필요한 정보를 추출하여, 응답하는것

![그림]


### 데이터셋
- SQuAD
  - stanford question answering dataset
  - 위키 피디아 아티글에 대한 질문-대답 데이터셋
  - [링크](https://rajpurkar.github.io/SQuAD-explorer/)

![그림]
- 지문들을 단어단위로 진행
  - 특정한 문구가 정답으로 지정됨
  - 해당 위치를 예측하는것을 학습
- 정답 문구가 어디서 시작하는지부터 예측을 시작
- 정답 문구가 어디서 끝나는지도 예측 시작
- 위 두개의 예측은 서로 다른 FC를 학습한다.

- SQuAD 2.0
  - 질문에 대한 답이 없는 것까지 포함
  - 답이 있는지 부터 판단
    - 있으면 답에 대한 문구를 예측
  - CLS토큰을 이용해 두개의 문장을 합치고 답이 없으면 0을 넣어준다.


### On SWAG
- 문장이 주어졌을때 다음이어질 적절한 문장을 찾는 task

![그림]
- 후보가 되는 각 문장마다 concat하여 BERT를 통해 나온 encoding vector의 결과를 본다.
  - 이렇게 나온 scala값을 softmax를 통해 정답을 추려낸다.


### 성능향상방법
- model은 크면 클수록 좋다.
- model을 크게 키우니 성능도 지속적으로 증가했다.


------


## Advanced Self-supervised Pre-training Models
- 좀더 진보된 self-supervised pre-training model들


## GPT-2
- transformer를 좀더 쌓았다.
- 40GB의 text를 학습
  - 데이터셋의 질을 높여서 학습
- down-stream tasks in a zero-shot setting에서 좋은 성능을 보임
  - down-stream이란
    - 구체적으로 풀고싶은 문제들
    - pretrain방식으로 학습을 진행, 이후 원하는 태스크를 fine-tuning하는 방식을 말한다.
  - zero-shot setting이란
    - [참고](https://blog.naver.com/sith_msip/221886769247)
    - 모델이 직접 학습데이터를 분류하고 정리해 카테고리를 형성하여 자체적으로 카테고리의 의미를 이해하는것
- 동기
  - decaNLP
  - The Natural Language Decathlon: multitask Learning as Qeustion Answering
    - 질의 응답으로 통합가능
- 데이타셋
  - prepocess
    - byte pair encoding(BPE)



## GPT3
- 개선함
- 모델 구조기보단, 모델사이즈를 더 많이 확장함
  - 많은 데이터, 큰 배치사이즈


## ALBERT
- a Lite BERT for Self-supervised Learning of Language Representations

## ELECTRA
- Efficiently Learning an Encoder That Classifies Token Replacements Accurately

### 예측하는방법
![그림]

- zeroshot
- oneshot
- fewshot





----

## 좀더 찾아보기
- transfer learning
- down-stream
- zero-shot setting
- byte pair encoding

-----



## 피어세션 정리
- 지난 과제 BPE 확인
  - 정규표현식
    - negative lookahead/lookbehind
- CLS는 어떤일을 하는건지?
  - CLS가 단순히 분류를 한다고 표현하는 토큰인지?
  - 아니면 학습할때 ground truth로 사용하는건지?
    - SQuAD, on SWAG
- 삼성 제조쪽으로 가자
  - 삼성 전자/후자로 
  - 삼성 sds/si는 후자도 아니라고한다.
- 텔레콤도 좋다.
- 갈수만 있으면 좋다~ 무야호
- [attention관련](https://medium.com/@yuvalpinter/attention-is-not-explanation-dbc25b534017)



### further question
- masked
  - masked가 있기때문에 어느정도 풍부해지지 못한다.


------

## 과제
- 정답코드 보면서 실제 돌리면서 확인
- 논문 읽는곳
  - facebook(tensorflow kr)
  - twitter(일부 연구자들)

------


## 마스터 클래스2
- 논문 읽기 좋은 사이트
  - [arxivsanity](http://www.arxiv-sanity.com/)
    - REALM
    - MEENA
    - UniLM
- 잘모르는것관련
  - 최소한 컨셉과 입출력 대해 이해를 하자.

## 회고
- 이번주부터 점점 어렵지만 재밌었다.
- 하지만 점점 이론보다는 코딩스킬을 갈고닦아야 할때가 온거같다
- 과제랑 실습을 보면서 코딩테스트 
- 다음주 graph가 걱정인다.
