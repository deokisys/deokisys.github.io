---
layout: post
title: 32.물체 탐지와 cnn시각화
categories: AI boost
tags: [ai,boost,object detection,R-CNN,Fast R-CNN,Faster R-CNN,YOLO,SSD,Focalloss,IoU,RoI,RetinaNet,AnchorBox,RPN,Selective Search,NMS,FPN,DETR,
cnn visualization,dimensionality reduction,Nearest neighbors,NN,t-SNE,layer activation,maximally activation patches,class visualization,gradient ascent,saliency test,via backpropagation,rectified unit,guided backpropagation,CAM,class activation mapping,global average pooling,GAP,Grad-CAM,Autograd]
toc : true
math : true
---


# 강의 복습 내용
- object detection
  - R-CNN
  - Fast R-CNN
  - Faster R-CNN
  - YOLO
  - SSD
  - Focalloss
  - RetinaNet
  - DETR
- cnn visualization
  - dimensionality reduction
  - analsis of model behaviors
    - Nearest neighbors
      - t-SNE
    - layer activation
    - maximally activation patches
    - class visualization
      - gradient ascent
  - model decision explanation
    - saliency test
    - via backpropagation
    - rectified unit
    - guided backpropagation
    - CAM(class activation mapping)
      - global average pooling(GAP)
    - Grad-CAM
- Autograd

## 얻은 지식

-----

## object detection
- 물체 하나마다 bounding box 단위 예측
  - ![그림](https://user-images.githubusercontent.com/24247768/110631598-091a9700-81ea-11eb-8791-8afa3630e3cc.png)
    - 분류와 box위치를 탐지
- two-stage detector
- one-stage detector

### two-stage detector
- gradient-based detector
  - ![그림](https://user-images.githubusercontent.com/24247768/110631659-1afc3a00-81ea-11eb-8433-1e4e785b94c2.png)
  - 경계의 기울기 기반 탐지로 이미지의 객체마다 가지는 기울기들을 보고 탐지하는 컨셉
  - HOG(histogram of Oriented Gradients)
  - SVM(Support Vector Machine)
- selective search
  - 다양한 물체에 대한 bounding box를 제안
  - ![그림](https://user-images.githubusercontent.com/24247768/110631731-2fd8cd80-81ea-11eb-9239-30a2db6a3c8f.png)
    - 잘게 영역을 만든다.
    - 비슷한 영역(기울기,색 등)을 기준으로 합친다.
    - 남겨진 영역을 표현하는 bounding box를 추출한다.

#### R-CNN
- ![그림](https://user-images.githubusercontent.com/24247768/110631790-4121da00-81ea-11eb-9205-8271e4cbcfdb.png)
  - 입력이미지에서 region을 추출
  - 각 region마다 크기를 조정하여 CNN적용
  - 결과를 분류
- region마다 cnn이 계산되어 오래걸린다.
- region proposal하는 부분이 따로 학습이 없다.

#### Fast R-CNN
- R-CNN의 느린방식을 개선
- 영상전체의 feature를 한번에 추출, 이feture를 활용
- RoI(region of interest)
  - region proposal이 제안한 후보 region을 의미
- ![그림](https://user-images.githubusercontent.com/24247768/110631859-526ae680-81ea-11eb-926a-764f35704eeb.png)
  - convolution을 걸쳐 feature map을 추출
  - RoI에 대해 featuremap에서 잘라서 roi pooling을 진행
  - 각 Roi마다 class와 box 탐지
- region proposal을 별도의 알고리즘을 쓰고있어 한계가 있다.

#### Faster R-CNN
- region proposal를 개선
  - neural로 적용
- end-to-end로 제작
- IoU(intersection over union)
  - ![그림](https://user-images.githubusercontent.com/24247768/110631968-6f9fb500-81ea-11eb-8b92-ca2038c3cff5.png)
  - 두 영역의 overlab을 측정하는 기준을 제공
  - 교집합/합집합
- AnchorBox
  - ![그림](https://user-images.githubusercontent.com/24247768/110632040-81815800-81ea-11eb-86b1-7166854fa24c.png)
  - 사전에 정의된 bounding box의 집합
  - 크기는 3단계, 비율도 3단계의 anchor box를 각 위치마다 생성한다.(다양하게 만드는건 가능)
  - IoU가 0.7넘으면 positive, 0.3보다 낮으면 negative
- RPN(region proposal network)
  - selective search보다 빠름
  - ![그림](https://user-images.githubusercontent.com/24247768/110632124-9b229f80-81ea-11eb-9d70-eeeac5720c26.png)
  - sliding window로 featuremap을 돌아다닌다.
  - 256차원의 feature map을 추출
  - k개의 anchor box를 적용
    - object인지 object가 아닌지에 대한 2k개의 score가 나온다.
    - bounding박스의 위치인 (x,y,w,h)정보를 나타내는게 4k만큼의 정보가 나온다.
- NMS(non Maximum suppression)
  - 여러 박스가 나오고, 중복도 많다.
  - 효과적으로 필터링하기위해 사용
  - 제일높은 점수의 박스를 선택
  - 나머지박스와의 IoU를 계산
  - 50을 넘는것들을 삭제
  - 이를 반복한다.

#### R-CNN요약
- ![그림](https://user-images.githubusercontent.com/24247768/110632174-a970bb80-81ea-11eb-95b8-53fd898fc798.png)


### one-stage detector
- RoI pooling이 없음
  - 좀더 빠르다.
  - two-stage는 각 roi마다 분류를 진행하게된다.
  
#### YOLO
- you only look ones
- ![그림](https://user-images.githubusercontent.com/24247768/110632244-bd1c2200-81ea-11eb-9b2b-a84e935245a7.png)
  - 이미지를 S x S로 그리드를 나눈다
  - 각 그리드마다 boundingbox와 confidence점수(object인지 아닌지인가) / class점수를 예측
  - NMS를 적용
- ![그림](https://user-images.githubusercontent.com/24247768/110632469-ffddfa00-81ea-11eb-9960-512454d89642.png)
  - 마지막 convolution 7x7로 grid가 나뉘어진다.
  - 거기에 따른 점수 5B+C를 계산
    - anchor를 2개사용하여 B는 2
    - 나오는 클래스는 20개여서 C는 20
    - 총 30이 나오며 마지막 convolution의 채널이 30인것을 확인 할 수 있다.
- 실시간으로는 성능이 좋았다.


#### SSD
- single shot multibox detector
- multi-scale구조를 만들었다.
  - ![그림](https://user-images.githubusercontent.com/24247768/110632855-711dad00-81eb-11eb-9b62-5441e4db84c4.png)
  - 8x8는 작은물체, 4X4는 좀더 큰물체를 본다.
    - 8x8은 큰 개를, 4x4는 작은 고양이를 본다.
  - 다양한 크기의 박스를 찾기위해 featuremap의 크기가 가변
  - 다양한 해상도에서 박스를 탐지
- ![그림](https://user-images.githubusercontent.com/24247768/110632990-95798980-81eb-11eb-9086-e9417349a3ab.png)
  - feature들이 layer를 거칠수록 점점 큰영역을 보게된다.
    - 다양한 scale을 대응
  - class(클래스점수)+4(박스정보)를 계산
    - 바운딩박스마다 계산이된다.
  - ![그림](https://user-images.githubusercontent.com/24247768/110633777-821aee00-81ec-11eb-858d-99711f4ce456.png)
    - 모든 anchorbox를 계산한것
    - 많은 anchorbox지만 구조가 단순해서 그런지 속도는 빠르다고한다.

### two와 one stage의 차이
- Focal loss
  - single은 roi가 없다보니 모든 영역에 대해 같은 loss가 적용
  - ![그림](https://user-images.githubusercontent.com/24247768/110633822-93fc9100-81ec-11eb-95c6-489f83232ab8.png)
    - negative예제가 너무많다.
    - positive예제는 너무 작다.
    - single detect의 문제
  - cross entropy의 확장
  - ![그림](https://user-images.githubusercontent.com/24247768/110633872-a8d92480-81ec-11eb-855b-b6bb9bdd17a2.png)
    - x틀리면 강한 loss, 맞추면 낮은 loss가 적용

- RetinaNet
  - FPN(Feature Pyramid Networks)
  - ![그림](https://user-images.githubusercontent.com/24247768/110633924-b68eaa00-81ec-11eb-8f44-43ce597458e1.png)



### Detection의 Transformer
- DETR
  - DEtection TRsformer
  - ![그림](https://user-images.githubusercontent.com/24247768/110634042-d6be6900-81ec-11eb-8416-47526415870e.png)
- Detection objects as points
  - 박스표현대신 중심점을 이용해 물체를 찾자
  - ![그림](https://user-images.githubusercontent.com/24247768/110634093-e9d13900-81ec-11eb-868a-f5cf356bf533.png)

------------

## CNN visualization
- black box모델인 cnn의 내부동작을 가시화하는 방법

### visualizing cnn
- cnn은 블랙박스
  - cnn내부는 뭐가있는지
  - 왜 성능이 좋은지
  - 성능 향상을 어떻게 해야할지
- 디버깅툴이라 생각하면된다.

#### filter weight visualization
- ![그림](https://user-images.githubusercontent.com/24247768/110634242-171de700-81ed-11eb-896f-fc9e491a517b.png)
- 각 필터를 시각화
- 이런 결과는 사람의 눈으로 제대로된 해석이 불가능하다.

#### 여러 형태의 neural network 시각화
- ![그림](https://user-images.githubusercontent.com/24247768/110634310-2b61e400-81ed-11eb-8875-a08bf9c81196.png)
  - analsis of model behaviors
    - 모델의 특성을 분석
  - model decision explanation
    - 출력을 분석
  - 왼쪽으로 갈수록 모델의이해, 오른쪽으로 갈수록 데이터를 분석


### analsis of model behaviors
- 모델의 특성을 분석

#### embedding feature analysis
- Nearest neighbors in a features pace
  - ![그림](https://user-images.githubusercontent.com/24247768/110640312-09b82b00-81f4-11eb-864e-00b0cdb9f294.png)
    - query이미지가 입력되면 그와 비슷한 6개의 이미지들을 출력
    - 강아지 예시에서 강아지 종류뿐만아니라 위차나 포즈가 달라도 잘찾아내는것을 볼 수 있다.
    - 어떻게 알수있을까
  - ![그림](https://user-images.githubusercontent.com/24247768/110640415-248a9f80-81f4-11eb-8848-0708bf374001.png)
    - feature space에서 NN을 사용한다
    - 쿼리 이미지의 featuremap에서 가까운것을 비슷한 feature로 보아서 결과를 나타낼수있다.
- dimensionality reduction
  - 너무 고차원이면 상상하기도 확인하기도 어렵다.
  - 고차원을 저차원으로 축소하는 방법
  - t-SNE(t-distributed stochastic neighbor embedding)
    - ![그림](https://user-images.githubusercontent.com/24247768/110640647-67e50e00-81f4-11eb-9d94-cf44bba7e5b5.png)
    - 2차원으로 표현


#### activation investigation
- layer activation
  - ![그림](https://user-images.githubusercontent.com/24247768/110641459-6405bb80-81f5-11eb-814b-7f2a15e54b8b.png)
  - 특정 layer의 적절한 thresholding을 통해 mask를 시각화
  - 이 hidden node가 무엇을 탐지하는지 보여준다.
  - 이를 통해 중간마다의 hidden node는 특정 뿐을 탐지하도록 되어 이런게 쌓여서 겨로가를 낸다고 생각할 수 있을듯
- maximally activation patches
  - ![그림](https://user-images.githubusercontent.com/24247768/110642387-5270e380-81f6-11eb-97df-67f1b4ccaa7d.png)
  - 각 hidden node의 최대인 부분에 대해 이미지를 보여준다.
  - hidden node가 어떤것을 보고있는지 분석이 가능
- class visualization
  - 네트워크가 기억하고있는것이 무엇인지 분석
  - ![그림](https://user-images.githubusercontent.com/24247768/110642632-8815cc80-81f6-11eb-95de-666e18b861ed.png)  
    - 아래보면 강아지뿐만 아니라 사람도 좀 보인다.
    - 강아지뿐만 아니라 사람도 일부 찾고있다는것을 볼 수 있다.
      - 학습데이터가 편향(bias)되어있다고 볼 수 있다.
  - 마지막 결과에 대해 분석
  - 빈 이미지를 입력, 역전파를 통해 좀더 높은 결과를 얻기위한 이미지를 지속적으로 업데이트한다.
  - gradient ascent
    - gradient descent와 똑같이 작동한다고한다.
  - ![그림](https://user-images.githubusercontent.com/24247768/110643095-0f634000-81f7-11eb-9781-8ce8e337412a.png)
    - 빈이미지를 모델에 돌려서 역전파를 진행하여 이미지를 업데이트
    - ![그림](https://user-images.githubusercontent.com/24247768/110643168-26099700-81f7-11eb-87a3-4a975e24112b.png)
      - 이를 지속적으로 반복하면 모델이 가지는 원하는 이미지를 출력하게 된다.

### model decision explanation
- 특정입력을 어떤각도로 보고있는지 분석

#### saliency test
- occlusion map
  - ![그림](https://user-images.githubusercontent.com/24247768/110643429-6e28b980-81f7-11eb-97b2-3d69e957d1d0.png)
  - a/b로 가려서 확률 계산
    - 물체의 중요한 부분을 가리면 점수에 큰 영향을 보여준다.
  - 이러한 결과를 heatmap으로 기록
- via backpropagation
  - ![그림](https://user-images.githubusercontent.com/24247768/110643602-9b756780-81f7-11eb-8f25-2131ce226e64.png)
  - 이미지 분류를 적용하고 나온 feature map을 확인
  - ![그림](https://user-images.githubusercontent.com/24247768/110643721-bfd14400-81f7-11eb-9810-fb72737f9188.png)
    - 이미지 입력, class score구하기
  - ![그림](https://user-images.githubusercontent.com/24247768/110643795-d24b7d80-81f7-11eb-8e18-4f0eb638102d.png)
    - 역전파를 진행하여 나온결과를 시각화(절대값 or 제곱) 이를 누적시킨다.
  - 어떤부분에 민감하게 보는지 볼 수 있다.
  - 입력한데이터에 의존적인 실험이다.

#### backpropagation-based saliency
- 연전파를 이용
- rectified unit(backward pass)
  - 정리필요
- guided backpropagation
  - ![그림](https://user-images.githubusercontent.com/24247768/110644362-5bfb4b00-81f8-11eb-96c5-bd0894f58243.png)
    - 고양이 이미지를 넣고 어디를 보고 고양이라고 인식했을까를 볼때
    - 눈과 귀부분이 명확하게 보인다
- class activation mapping(CAM)
  - ![그림](https://user-images.githubusercontent.com/24247768/110644730-bd231e80-81f8-11eb-9da9-ad38f9735563.png)
  - ![그림](https://user-images.githubusercontent.com/24247768/110644762-c57b5980-81f8-11eb-8c5f-bec65618f14f.png)
  - 뒷부분을 FC layer대신에 global average pooling(GAP)layer로 교체
  - ![그림](https://user-images.githubusercontent.com/24247768/110644876-ddeb7400-81f8-11eb-9e86-a7ae52f45924.png)
    - GAP을 통해서 계산되는것을 식으로 표현
    - 모든 채널 k를 돌면서 채널마다 클래스에대한 가중치(w)와 feature(f)를 합성곱하여 더한다.
    - F는 채널마다 모든 픽셀에 대한 평균이라한다.
  - ![그림](https://user-images.githubusercontent.com/24247768/110644935-ea6fcc80-81f8-11eb-9d65-de7045245310.png)
    - CAM유도
    - 위의식에서 순서를 바꿔준것
    - global average pooling을 이용하기전
    - 공간의 대한 정보를 가진다고한다.
  - ![그림](https://user-images.githubusercontent.com/24247768/110645005-fa87ac00-81f8-11eb-840f-a581283a11b7.png)
    - 공간에대한 정보가 보인다.
    - 공간정보를 따로 주지않아도 물체 위치를 탐지도 가능하다.
  - 마지막 레이어가 gap과 fc layer로 이루어져있어야한다.
    - 마지막구조를 바꾸서 재학습이 필요하다.
- Grad-CAM
  - ![그림](https://user-images.githubusercontent.com/24247768/110645139-0ffcd600-81f9-11eb-9add-8b44c09369d3.png)
  - 모델구조 변경없이 사용가능
  - 백본이 cnn이면 사용가능
  - ![그림](https://user-images.githubusercontent.com/24247768/110645240-24d96980-81f9-11eb-9f54-b1bccd4a57fc.png)
  - ![그림](https://user-images.githubusercontent.com/24247768/110645448-4fc3bd80-81f9-11eb-8764-a01ea0231bb4.png)
    - alpha는 기존과 다른 weight
    - 관심이 있는 convolution까지 역전파를 진행한것을 gloval average pooling한다.
      - 이를 activation map과 결합하기 위한 weight로 사용
  - ![그림](https://user-images.githubusercontent.com/24247768/110645598-7550c700-81f9-11eb-84cb-d45e7ed849c0.png)
    - activation map과 선형결합하여 Relu를 한다.


### Autograd
- automatic gradient calculating API


----

## 좀더 찾아보기
- RetinaNet
- rectified unit(backward pass)

-----


## 피어세션 정리
- weight가 transpose되는부분
  - nn에서는 weight가 transpose가 자동으로 적용된다.
- 내일 성익님 못오심
- 상진님 중단



