---
layout: post
title: 37.acceleration,pruning
categories: AI boost
tags: [ai,boost,acceleration,가속화,pruning,가지치기,LLVM,lottery ticket hypothesis]
toc : true
math : true
---

# 강의 복습 내용
- 가속화
- 가지치기
  - lottery ticket hypothesis

## 얻은 지식

-----

## acceleration
- 언어->어셈블리어->기계어 
- 머신러닝도 단계가 있다.
  - high-level IR   
    - pytorch, tensorflow,...
  - DL compiler
    - Glow,XLA,TVM,...
  - Low-level IR
    - LLVM

### 가속
- acceleration
- list와 numpy array차이는?
  - list는 각 요소를 따로 오브젝트로 저장
  - numpy는 비슷한위치에 저장
  - numpy는 c베이스로 제작되었다.
- bandwidth
  - 대역폭, coummunication link가 다루는 데이터 용량(잠재적인 성능)
- latency
  - 주파수, source에서 destiny까지 완전히 넘어가는동안 걸리는시간
- throughput
  - 처리율, 시간당 데이터 처리양(실제가능한 데이터 처리율)
- parallel processing(병렬 처리)는 thoughput만 증가해준다.
- ![머신러닝그림](https://user-images.githubusercontent.com/24247768/111445018-50ef7000-874e-11eb-90bc-af6903aa4731.png)



### 하드웨어(칩)
- ![그림](https://user-images.githubusercontent.com/24247768/111445089-649ad680-874e-11eb-9080-55278aa0146e.png)
  - hardware acceleration은 cpuㅇ에서 돌릴때보다 하드웨어적으로 좀더 효율적인 성능을 이끌어내길 원할때 사용한다.
    - latency를 줄이고, throughput을 늘린다.
- processing units
  - 많은 pu들이 있다.
  - ![그림](https://user-images.githubusercontent.com/24247768/111445152-74b2b600-874e-11eb-8fe5-67c8ac31ec05.png)
    - TPU, NPU는 딥러닝관련해서 만들어진 것들.
- [cpd와 gpu차이](https://www10.mcadcafe.com/blogs/jeffrowe/2017/03/16/the-continuing-importance-of-gpus-for-more-than-just-pretty-pictures/)
  - ![그림](https://i2.wp.com/www10.mcadcafe.com/blogs/jeffrowe/files/2017/03/CPU-and-GPU.png?w=925&ssl=1)
- arm
  - cpu설계하는 회사
  - 모바일에 많이 사용
- SoC
  - System on chip
  - 하나의 칩에 다 넣은것
- FPGA
  - field programmable gate array
  - 조립되는 칩
  - 아두이노 비스므리한것

### 압축과 가속
- compression and acceleration
- compression
  - 소프웨어적인 내용
  - 필요한내용 불필요한 내용 자르는등
  - 공간적
- acceleration
  - 하드웨어적인 내용
  - 처리속도,효율 올리기 등
  - 시간적
- 하드웨어의 성능
  - acceleration
  - 하드웨어는 생각한것을 수행해주는 기계
  - 하드웨어의 성능은 처리속도를 높이는것에 초점을 맞춘다.
- 하드웨어와 소프트웨어 둘다 같이 고려해서 압축을 해야할것이다 라고 한다.

### deep learning compiler
- 일반적인 언어(파이썬)로 만든 딥러닝 코드를 TPU같은 가속기에 올리기위해 변환
- compiler
  - high level -> assembly level로 변환하는것
  - high level language
    - c++,python,java
  - assembly language
    - arm, mips, x86
  - machine language
    - 2진수
- 컴파일러도 기업마다 다르다.
  - XLA,TVM,GLOW,ONNC
- LLVM
  - high level언어와 assembly언어로 변환해주는 compiler
  - 마치 jvm같이 통합하여 변환해준다.
  - MLIR
    - LLVM을 발전시킨 형태라고 본다.
- DL compiler
  - DL model(pytorch 등)이 입력
  - compiler fronted
  - compiler backend
  - target platforms(cpu, gpu등등)에 맞게 출력
- 개발에 유익하기보단, 회사에서 의사결정과 요구사항에 맞는 프레임워크를 조사(survey)할때 조사
- 지역적인특성
  - temporal 한번쓴건 최근에 또 사용
  - spatial 사용한것과 근처에 있는 데이터에 접근확률이 높다.
  - branch 분기될 가능성은 현재 위치에 의존적이다는 의미? 
  - equidistant 반복적으로 되는것은 특정 패턴이 있어서 예측이 가능하다.

-------

## pruning for network comression
- 가지치기
  

### weighted sum
- decision theory에서 
  - weight가 높은건 더 높게, 낮은건 더 낮게 가중해준다.
- 가중치를 주는것

### pruning
- 중요한것은 살리고, 덜중요한것은 잘라낸다.
- 장점
  - 추론속도가 빨라진다.
  - 모델복잡도가 줄어든다.
  - 일반화가된다.
- 단점
  - 정보를 잃는다.
  - the granularity affects는 하드웨어 가속화 디자인에 영향을 미친다고한다(?)
- ![그림](https://user-images.githubusercontent.com/24247768/111445338-a9bf0880-874e-11eb-9dd4-30f29d7dd4b3.png)
  - pruning 방식
  - 자르고 retrain하여 올리고, 다시 자르고 재학습
- 딥러닝의 pruning
  - ![그림](https://user-images.githubusercontent.com/24247768/111445482-ca875e00-874e-11eb-9a8c-f351853985e8.png)
- pruning과 dropout
  - 둘다 중간 weight를 날리는것
  - pruning은 복원을 안함, dropout은 train할때 껏다키면서 앙상블

### 여러 pruning들
- global magnitude pruning,...등등
- 너무많음
- 카테고리화
  - 자주쓰는지, 언제할지, 어떻게 할지, 무엇을할지
- unstructured과 structured
  - ![그림](https://user-images.githubusercontent.com/24247768/111445565-decb5b00-874e-11eb-9529-576088a841d7.png)
  - 특정 weight만 자르기 / 특정 노드전체를 자를지
  - 특정 규격을 정해서 자른다.


### lottery ticket hypothesis
- 로또로 가지쳐서 좋은게 나온다는 이론
- ![그림](https://user-images.githubusercontent.com/24247768/111445785-14704400-874f-11eb-8158-55d85bae2374.png)
  - 네트워크의 서브 네트워크인데 오리지널보다 좀더 좋은 네트워크가 있다는 가설
    - pruning전에 얻은 정확도가 pruning하고난 모델로 더 좋은 모델이 있다는 가설
- 제대로된 증명이 없다
- lottery를 위해 training을 해줘야한다.




----

## 좀더 찾아보기
- lottery ticket

-----


## 피어세션 정리
- 가속화의 제목의 의미?



