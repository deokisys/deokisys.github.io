---
layout: post
title: 38.양자화와 knowledge distillation
categories: AI boost
tags: [ai,boost,양자화, knowledge distillation,quantization]
toc : true
math : true
---

# 강의 복습 내용
- 양자화
  - float표현법
  - quantization
- knowledge distillation

## 얻은 지식

-----

## 양자화
- 실수형태를 특정단위로 잘라서 사용

### fixed-point, floating-point
- fixed(32비트)
  - 소수부의 자릿수를 고정
  - 구조
    - 1비트는 부호
    - 8비트는 정수(integer part)
    - 23비트는 소수(fractional part)
- floating-point
  - 고정보다는 넓은범위
  - 구조
    - 1비트는 부호
    - 8비트는 지수
    - 23비트는 가수부
- FPU(floating point unit)
  - 소수계산전문 유닛

### quantization
- 양자화
- 모델크기가 줄어든다.
- 표현력은 줄어든다.
- 추론속도가 빨라진다.
- ![그림](https://user-images.githubusercontent.com/24247768/111609509-b6f6f880-881d-11eb-865a-95a860a4a1af.png)
  - 4비트는 16층으로 이루어져 bit로 4bit가 된다.
  - 3비트는 8층으로 이루어져 bit로 3bit
  - int는 -127부터 127까지 5층으로 표현이된다.
  - 위 그래프의 색이 칠해진게 에러라고 볼수있다.
  - float으로 양자화를 적용하여 int로 변환하면 float의 적은범위가 int에서는 큰 범위로 적용되므로 손실이 발생
- 양자화 하는 방식
  - ![그림](https://user-images.githubusercontent.com/24247768/111610386-a72be400-881e-11eb-9a47-fd76237b5f35.png)

### 양자화 종류
- ![그림](https://user-images.githubusercontent.com/24247768/111610481-c0349500-881e-11eb-8218-fc4fac93a36e.png)
  - diynamic
  - static
  - quantization awaer training(qat)가 있다.

-----

## knowledge distillation
- 지식 증류
- 큰 모델이 학습한것을 작은곳으로 증류하여 넘기는 컨셉
- ![그림](https://user-images.githubusercontent.com/24247768/111607587-bcebda00-881b-11eb-9b3b-df0dd931b942.png)
  - teacher, student모델학습에서 사용된다.



----

## 좀더 찾아보기
- 양자화란?
  - 양자화 종류들
- knowledge distillation

-----


## 피어세션 정리
- 강의 후기
- 마스터세션 질문 정리



