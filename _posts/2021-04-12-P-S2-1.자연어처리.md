---
layout: post
title: P-S2-1.자연어처리
categories: AI boostP
tags: [ai,boost]
toc : true
math : true
---

## 강의확인

### 자연어처리
- 인간
  - 인코딩(나무라는 객체인식->tree라고 말함)
  - 디코딩(tree를 들음->나무라는 객체 인식)
- 컴퓨터
  - 인코딩(text-> 벡터 형태)
  - 디코딩(벡터->text)
- 분류
  - 의미(질문인지), 객체(무엇을 말하는지),...
  - 특징(feature)를 파악하여 그래프위에 이쁘게 분류가 된다면, 새로운 문제에대해서도 특징에따라 분류가 가능해진다.
    - word2vec을 이용한다.
 - one-hot - 해당 단어의 벡터만 1로 
    - sparse해서 단어가 간지는 의미를 벡터공간에서 파악하는게 불가능
    - 이를 보완하기위해 word2vec
- word2vec
  - 한 단어의 주변 단어들을 통해 그단어의 의미를 파악(skip-gram)
    - 알수없는 단어 옆에 멍멍있으면 개라고 파악하는 느낌
  - 이를 통해 단어가 가지는 의미 자체를 다차원 공간에 '벡터화'
  - 단어간의 유사도 측정, 관계파악, 벡터연산으로 추론이 가능
  - 단어간의 subword information을 무시
    - 서울,서울시,고양시가 비슷하다고 파악한다.
  - out of vocabulary 학습에 사용된 단어가 아니면 적용 불가능이다.
- fasttext
  - subword information을 위해
  - 주변단어와 중심단어에서 n-gram을 사용
    - 하나의 단어를 n크기로 분리하여 학습을 진행
    - oranges를 학습하면 orange라는 단어와 유사하다는것을 알 수 있게 된다.
      - o,or,ra,an,... es,s와 o,or,ra,...ge,e 이둘의 분리된 단어가 유사하니까
  - 오탈자, OOV,등장회수가 적은 학습 단어에 대해 강세
- word2vec,fasttext단점
  - 동형어와 다의어에대해서는 성능이 좋지않다.
  - 문맥을 고려하지 못한다.
  - 이를 위해 언어모델을 만든다.

### 언어모델
- 모델이란 현재상태를 통해 미래상태를 예측
- 언어모델이란 자연어의 법칙을 컴퓨터로 모사한 모델
- markov기반 언어모델
  - 마코프 체인 모델
  - 다음의 단어나 문장이 나올 확률을 계산
  - 이게 RNN
- RNN
  - 이전 state가 다음 state를 예측하는데 사용, 시계열 데이터 처리에 특화
  - 마지막 출력은 앞선 단얻ㄹ의 문맥을 고려해 만들어진 최종 출력 vector가 나온다.
    - 여기서 분류를 적용하면 문장 분류가 가능
- seq2seq
  - encoder layer, decoder layer
    - encoder는 rnn으로 context vector를 흭득
    - decoder는 context vector입력으로 출력
- attention
  - rnn은 매우긴 sequence는 앞쪽의 단어의 정보는 희석된다.
    - context vector는 길이가 한계가있다.
    - 긴 문장은 정보를 함축하기 어렵다.
    - 모든 token이 영향을 미친다.
  - 인간과 같이 중요한 feature에 집중하는 컨셉을 적용
  - rnn은 순차적으로 연산이되어 속다가 느림
- transformer
  - self attention
  - 이전 state를 다음 state로 넘기는 구조 제거
    - 한번에 사용

## 피어세션
- 자기소개
- 아이디어
  - 청각장애인 수화 번역기
