---
layout: post
title: P-S2-3.BERT(1)
categories: AI boostP
tags: [ai,boost]
toc : true
math : true
---

## 강의확인

### BERT
- transformer를 사용
- masking된 입력을 원본내용으로 돌리는것을 학습
  - 입력에 masking이 들어가서 어려운 문제가 된다.
- 입력을 마스킹, 교체, 변화없음 3가지로 랜덤하게 학습된다.
- NLP실험
  - GLUE
  - 단일문장분류, 두문장 관계, 문장토큰 분류, 기계 독해 정답 분류
- 단일 문장분류
  - 감정 분석 - 긍정인지 부정인지
  - 관계 추출
    - 하나의 문장에 관계가 존재(세르반테스는 소설 돈키호테를 썼다.)
- 두 문장 관계
  - 의미비교 - 두 문장의 의미가 유사한지
- 문장 토큰 분류
  - 개체명 분석
- 기계 독해 정답 분류
  - 기계 독해 - 질문,답이 합쳐서 입력
- 한국어 BERT
  - ETRI KoBERT
    - 한국어에 맞게 형태소 분석, wordpiece를 이용해 학습


### 실습
- 허깅페이스 라이브러리
- 유사도
  - cosinesimilarity가 정답은 아니다.

### 더 볼것
- wordpiece란?
- 허깅페이스확인
- bert복습

-----

### 피어세션
- wordpiece가 무엇일지?
  - bytepair를 확인
- 엔티티토큰을 넣어볼 생각


## 오늘 한일
- 챗봇실습


## 어떻게 했는지
- 실습으로 진행하는 코드를 활용해서 챗봇 학습을 진행


## 좋았던 점
- 챗봇이 어떻게 보면 간단한 코드로 만들어지는게 흥미로웠다.


## 아쉬운 점
- 완벽한 챗봇은 아니고, 성능이 그렇게 좋은 편은 아니었다.
- 이런 성능 개선을 위해 어떻게 할지가 제대로 떠오르지 않았다.
