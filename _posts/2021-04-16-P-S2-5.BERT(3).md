---
layout: post
title: P-S2-5.BERT(3)
categories: AI boostP
tags: [ai,boost]
toc : true
math : true
---

## 강의확인

### 의존구문 분석
  - 단어들 사이의 관계를 분석
  - 의존소, 지배소로 나뉜다.
    - 지배소는 의미의 중심
    - 의존소는 지배소가 갖는 의미를 보완해주는 요소이다.
  - 복잡한 자연어구조를 그래프로 구조화해서 표현이가능해진다.
  - 대상에 대한 정보 추출이 가능해진다.
### 문장분류
- 주어진 문장이 어떤 종류의 범주에 속하는지 구분
- task
  - 감정분석, 주제라벨링(범주화) 
  - 언어감지(어느나라언어인지), 의도분류(문장의 의도, 질문, 불만, 명령)
- 데이터셋
  - kor_hate(혐오표현),kor_sarcasm(비꼬기)
  - kor_sae(예,아니오 답변되는 질문, 명령)
  - kor_314k(의도,질문)
- 모델(bert)
  - ![그림]
  - cls토큰의 vector를 분류(classification)를 진행하여 dense layer사용



-----

### 피어세션
- 코일렉트라해봤는데 별로였다.
- training arguments설정
  - 중간에 검증하는것도 따로 존재하다.
  - load best model이라고 중간에 최고인것을 저장한다고 한다.
    - metric for best model
  - fp16
    - mix하는거라 좋았다.
  - label smoothing해봤는데 떨어졌다.
- loss수정은?
  - Trainer객체 상속해서 compute_loss메서드를 오버라이딩


### 오피스아워
- GPT, 크로스아워?
- brain inspired ai(뇌의 정보처리 방식)
- relation extraction
  - 문장에서 특정 단어들에대한 속성과 관계를 추론
  - 코드설명
- bert말고 다른 언어모델이 존재하니 시도해보기
- 특정 스페셜 토큰을 넣어보기
  - entity embedding layer를 추가해서 진행해보기(아직 뭔소린지 모름)
  - 이순신과 조선에 sub, obj토큰을 넣어보거나 해보는방식
- text augmentatino
  - pororo라이브러리
  - 번역하고 다시 한국어 번역해보기
- single로 볼지, multi로 볼지?
  - 자신의 전략이 된다고한다.
- trainer로 학습하는것이 간단한 방식이다.
- 토크나이저따라 성능이 달라진다.
  - 토크나이저란?
  - 토크나이저 직접만든다고한다.
- 높은 성능보다는 다양한 시도를 해보기
  - 단순히 epoch, learning-late보다는 다양한 시도.

------

## 오늘 한일
- base코드에서 pretrained적용해보기
- 새로운모델 kykim/bert-kor-base적용

## 어떻게 했는지
- 현재 베이스코드에서 pretrained가 안되어있는것을 변경
- 한국어 특화된 새로운 모델 kykim/bert-kor-base를 사용


## 좋았던 점
- 처음으로 변경해서 성능이 올랐다.
- 아무래도 한국어에 특화된 bert라고 명시된 모델이라 그런지 성능이 오른듯 하다.

## 아쉬운 점
- 내부적으로 직접 코딩을 해조고 싶은데 제대로 분석을 못하였다. 
- 이를위해 tranformer의 사용법을 제대로 익혀야 한다.