---
layout: post
title: P-S2-8.GPT
categories: AI boostP
tags: [ai,boost]
toc : true
math : true
---

## 강의확인


### GPT
- transformer의 decoder를 사용한 모델
- 학습방법
  - `발 없는 말이 ~~`라는 문장이 있을때
  - `발`이후 나오는 단어가 무엇인지를 예측 - `없는`예측
- GPT1은 bert보다 먼저나옴
  - rnn과같이 문장이 입력으로 왔을때, 문장의 context vector를 출력, 그 뒤에 linear레이어로 문장 분류를 위해 설계됨
  - 자연어 분류에 성능이 좋다.
  - 문장을 해석하면 따로 finetunning이 필요없을지도...
- fine tunning
  - 데이터를 넣으면서 gradient를 업데이트를 진행
- gradient 업데이트 없이 fine tunning
  - few shot, one shot, zero shot
    - 예측에 필요한 힌트를 조금줄지, 하나줄지, 아예 안줄지 
  - 학습한 모델이 주어진 힌트를 활용하여 예측성능 향상
- 


------

## 피어세션
- xlm roberta로 변환할때 special토큰을 제대로 변환해야 한다고한다.
  - 동작함

------

## 오늘 한일
- wandb적용
- train을 train,val셋으로 나누기 사용

## 어떻게 했는지
- wandb 홈페이지 참고
  - wandb install
  - wandb 설정
- train을 나누기위해 sklearn.model_selection의 train_test_split을 사용
  - seed고정


## 좋았던 점
- 이전엔 하이퍼파라메터만 건드리다가 이전에 못했던 train_test를 분리해볼수있었다.

## 아쉬운 점
- 성능은 전체 데이터 셋을 사용한것보다 낮았다.
- 다음엔 k-fold를 도전해보려한다.
