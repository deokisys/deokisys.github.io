---
layout: post
title: 11.베이즈정리와 딥러닝 기초와 PyTorch
categories: AI boost
tags: [ai,boost,베이즈정리,pytorch]
toc : true
math: true
---

# 강의 복습 내용
- 베이즈 정리와 딥러닝의 기초를 공부한다.
- pytorch를 통한 mlp를 해석한다.


## 얻은 지식
- 베이즈정리
- pytorch의 특징
- 딥러닝 기초, 역사

----

## 베이즈 정리
- 어떤식으로 모델의 모수를 추정, 모델 추정하는데 사용되는 베이즈 정리
- 데이터가 추가될때 데이터를 업데이를 하는방식에 이론적으로 정리
- 기계학습의 모델예측에 자주 사용되는 이론과 방법론이라고 한다.
- 두 확률변수의 사전 확률과 사후 확률 사이의 관계를 나타내는 정리


### 조건부확률
- P(A 교집합 B) = P(B)P(A|B)
  - 곱하는 형식
- P(B|A)=P(A교B)/P(A) = P(B)*(P(A교B)/P(B))

### 베이즈 정리 예
- $$p(\theta|D) = p(\theta)*\frac{p(D|\theta)}{p(D)}$$
- 사전확률, 민감도, 오탐율, 정밀도를 구하는 문제
- 사후확률, 가능도

### 조건부확률,인과관계
- 조건부 확률을 인과관계를 추론할때 함부로 사용하면 안된다.
- 인과관계는 데이터 분포의 변화에 강건한 예측모형을 만들 때 필요







## 딥러닝 기초(pytorch)
- linear algebra, probability의 수학스킬

### 기초
- AI-ML-DL(뉴렬 네트워크)
  - DL은 AI의 일부이다.

- 딥러닝의 핵심
  - 학습할 만큼의 데이터
  - 모델
  - loss함수
  - optimizer algorithm - loss를 줄이기위한 알고리즘

### 역사
- 2012년에 alexnet이 등장
  - cnn
  - 244*244를 분류하는게 목적
  - 딥러닝을 이용해서 1등
- 2013년 DQN
  - 강화학습
- 2014년 encoder/decoder
  - 다른언어의 연속을 주어질때, 또다른 언어의 연속을 만들어내는것
- 2014년 adam optimizer
  - 대부분 사용
  - 결과가 제일 좋음
  - 왠만하면 잘된는 최적화
- 2015 Generative Adversarial Network
  - 두개의 네트워크를 학습
- 2015 Residual Networks
  - 네트워크를 깊게 쌓은 딥러닝을 보여줌
  - 너무 깊으면 안좋다는 인식을 깼다.
- 2017 transformer
  - "attention is all you need"
  - attention
- 2018 BERT
  - language model
  - fine tuneed nlp models
- 2019 gpt-3
  - bert의 끝판왕
  - language model
  - 많은 prameter가 필요
- 2020 self supervised learning
  - simCLR
  - 한정된 학습데이터의 모델과 lossfunction을바꾸기보단
  - 학습데이터이외의 라벨을 모르는 데이터를 분류하겠다.

## PyTorch
- 대부분 어디서 만들어진 프레임워크, 라이브러리를 사용한다.
- TF는 값을 주입, PyTorch는 불러올때 바로 사용
  - static graphs/dynamic computation graphs
- tf
  - session으로 판을 만들고 feed로 값을 주입하는 구조
  - 코딩과 디버깅이 많이 복잡함
- Numpy + AutoGrad + Function
- numpy구조를 가지는 tensor 객체로 array표현
- 자동미분 지원
- 다양한 형태의 DL을 지원하는 함수와 모델 지원

### 설치
- colab을 통해 수행
- vscode와 연결방법
  - 주의 cloudflare의 위치를 정해줘야 한다.

### 드라이브내의 파일 접근 오류
- drive mount를 하여 토큰을 받고 인증을 하면 폴더를 확인할 수 있게 표시된다.
- os.getcwd()를 하면 위치 확인이 가능
  - 현재위치가 `/content`로 나올것이다.
  - 위치가 현재 작성하는 파일의 위치가 아니어서 생기는 문제
  - os.path.join()을 이용하여 위치를 특정해준다.
    - `os.path.join("/content",...,"flag.npg")`



## 뉴렬 네트워크 - MLP
- muulti-layer perceptron
- 인간처럼 생각할 수 있게
- 날고싶은 욕망은 새를 모방하여 날개를 움직이게 되었지만 현재 비행기와 많이 달라졌다.
  - 뇌를 모방해서 좋은게 아닌, 모델을 분석하여 성능에 대한 계산이 필요하다.
- 함수를 근사하는 모델, 비선형 연산이 반복적으로 일어나는 모델

### linear neural networks
- 입력이 1차원 출력이 1차원인 문제
  - 선형회귀문제
  - 기울기, 절편, 두개이 parameter로 이뤄짐


----

## 좀더 찾아보기
- 통계관련
  - 민감도 recall
  - 오탐율 false alarm
  - 정밀도 precision
- 딥러닝
- 모듈
  - PIL - 이미지
  - gunzip - gz파일 압축해제
  - torchinfo,torchsummary
    - 모델 구조 확인
-----

## 피어세션 정리
- 목요일 조교님
  - 수요일까지 질문생각
- 마스터세션 후기
  - 학습 단계
  - 풀스택 딥러닝
- 베이즈정리 좀더 확인
  - 빈도주의
    - 이미 결정된 데이터를 가지고 통계
  - 베이즈
    - 주관성을 모델에 포함시킨다. 확률을 아직 데이터로 검증되지 ㅇ낳은 종류까지 임의의 확륭을 포함해서 사용
    - 베이즈 조건을통해 사후 확률을 통해 특정 확률로 수렴
    - 예측했던게 잘못되어도 학습을 통해서 
    - 귀납적, 데이터를 모일수록 
  - 업데이트 
    - 해석할때.
    - 모델을학습하는 과정에서 해석을 하는 시야를 넓히는것
    - auto encoder의 모든것

  - 조건부는 오버피팅
  - 인과관계는 강건한  예측한모델.
- gpu의 스레드 dataloader의 num_worker
- kaiming_normal
  - 정규분포
- pytorch의 detach
  - 추적하는 옵션을 끈다
    - 무엇일까
  - 수정하면 원본도 수정한다.
- pytorch view(-1,크기)
  - -1로 남은 크기는 알아서 조정한다.
  - broad cast
- 별도 진행 코드는 colab공유
  - 빈칸채우기는 다같이 코드를 보면서 진행


## 의문
- 베이즈정리가 ai에 어떻게?
- 인과관계와 조건부확률의 차이는?
- 인과관계가 더 안정적인게 무엇인지?

----

## 과제
- MLP 빈칸 채우기
- notMNIST데이터
  - http://yaroslavvb.blogspot.com/2011/09/notmnist-dataset.html
  - http://yaroslavvb.com/upload/notMNIST/notMNIST_large.tar.gz
  - tar -xvf 파일로 압축해제
  - notMNIST데이터를 다루는 dataset calss만들기