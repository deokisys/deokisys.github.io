---
layout: post
title: 14.RNN과 LSTM
categories: AI boost
tags: [ai,boost,RNN,BPTT, LSTM, transformer,MHA]
toc : true
math : true
---


# 강의 복습 내용
- RNN의 정의
  - 역전파 BPTT
- RNN과 LSTM의 차이
- LSTM의 계산 방식
- transformer와 LSTM의 차이
- transformer의 MHA는?


## 얻은 지식
- RNN
  - recurrent neural network
- BPTT
  - 오래된 과거가 약해진다.
  - 중간에 끊어주는 과정이 필요
- LSTM
- transformer
  - MHA(multi-headed attention)

-----

## RNN
- recurrent neural network
- 시퀀스, 시계열 데이터를 처리하는데 사용
  - 독립적이지 않은 연속적인 데이터들

### 시퀀스 데이터
- 소리, 문자열, 주가 등의 데이터를 시퀀스 데이터로 분류
  - 문자는 문법과같은 규칙을 통해 의도들이 들어있다.
- 독립동등분포(i.i.d) 가정을 잘 위배한다.
  - 순서가 바꾸거나, 과거 정보에 손실이 발생하면 데이터의 확률분포도 바뀐다.

- 조건부 확률을 이용
  - 이전 시퀀스의 정보를 가지고 앞으로 발생할 데이터의 확률분포를 다룸

$$ p(x_1,x_2,\cdots) = p(x_t\mid x_{1},\cdots,x_{t-1})p(x_1,\cdots,x_{t-1}) $$  
$$  = p(x_t\mid x_{1},\cdots,x_{t-1})p(x_{t-1}\mid x_{1},\cdots,x_{t-2})\cdots $$  
$$  = \prod\limits_{\rm s=1}^{\rm t}{p(x_s\mid x_{s-1},\cdots,x_1)} $$


  - 초기시점 x1부터 바로 직전의 과거시점 xt-1까지의 정보를 사용
  - 현재시점인  Xs를 모델링하여 모두 곱해준다.

$$ X_t = P(X_t|X_{t-1},\cdots,X_1) $$


  - x1부터 xt-1까지의 정보가 조건부로 주어지고
  - 현재 정보인xt를 모델링하는 조건부 확률 분포를 모델링
  - 시퀀스데이터를 다루는 가장 기본적인 방법론
- 과거의 모든게 필요한게 아니다.
- 어느정도 과거의 정보를 정해서 사용해야한다.

### 시퀀스 데이터 다루기
- 길이가 가변적인 데이터를 다룰 수 있는 모델이 필요

$$ X_t \sim P(X_t|X_{t-1},\cdots,X_1) $$

  - 모든과거 x1부터 xt-1

$$ X_{t+1} \sim P(X_t+1|X_{t},\cdots,X_1) $$

  - 모든과거 x1부터 xt까지
  - 양이 너무 많다.
  - 위 수식처럼 모든 과거를 볼 필요가 없다.

$$ X_{t+1} \sim P(X_t+1|(X_{t},\cdots,),\cdots,X_1) $$


- 괄호친 고정된 길이 $$ \tau $$만큼의 과거만큼의 시퀀스만 사용
  - AR($$\tau$$)(autoregressive Model)자기회귀 모델
  - 하이퍼 파라메터이어서 정하는기준이 필요 하다.


$$ X_{t+1} \sim P(X_t+1|X_{t},(\cdots,X_1)) $$
$$ X_{t+1} \sim P(X_t+1|X_{t},H_{t+1}) $$
$$ H_t = Net_\theta(H_{t-1},X_{t-1}) $$
- 잠재변수 H
  - xt+1일때 xt를 제외한 과거의 정보인 x1부터 xt-1까의 정보정보들을 H라는 잠재 변수로 인코딩 해서 활용하는 잠재 AR모델
 
- 이러한 H를 신경망을 통해 반복해 시퀀스 데이터의 패턴을 학습
  - 이러한 모델이 RNN

### RNN
- WX(1),WH(1),W2(1)
  - t에 따라 변하지 않는다.
  - 각각에 t에 따라 똑같이 적용된다.

### 역전파
- BPTT(backpropagation through time)
- 과거가 많을수록 불안정하다.
  - 역전파가 0으로 사라진다고 한다.
  - truncated BPTT
  - 길이를 끊는다.
  - vanilla rnn은 길이가 긴 시퀀스를 처리하는데 묹가 있다.
  - 이를 해결하기 위해 lstm, gru가 있다.

-------

## RNN
- 입력 데이터가 Sequential data이다.
- Recuurent Neural Networks에 대한 정의와 종류

### sequential model
- sequential data를 처리하는 어려움

- Naive sequence model

$$p(x_t|x_{t-1},x_{t-2},\cdots) $$

  - 받아 들이는 데이터의 입력이 어느정도인지 파악이 불가
  - 과거의 정보들이 점점 늘어난다.

- autoregressive model

$$p(x_t|x_{t-1},\cdots,x_{t-r}) $$

  - 과거의 일부만 보기로 한다.

- markov model(first-order autoregressive model)

$$ p(x_1,x_2,\cdots) = p(x_T\mid X_{T-1})p(x_{T-1}\mid X_{T-2}),\cdots $$
$$= \prod\limits_{\rm t=1}^{\rm T}{p(x_t\mid x_{t-1})} $$
  
  - 바로 직전의 과거만 본다.

- latent autoregressive model
  - 기본적인 문제는 너무많은 과거
  - hidden state라는 과거의 정보들을 요약한것을 이용

### RNN
- 자기자신에게 돌아오는 구조
- 단점 
  - short-term dependencies하다.
  - 하나의 고정된 rule로 사용되기 때문에 과거의 일이 미래에 점점 영향이 떨어진다.
  - 이를 해결하기위해 lstm이 있다
- vanishing/ exploding gradient
  - 가중치가 과거에서부터 같은 식으로 반복적으로 계산하니 생기는 문제
  - 과거의 일이 약해지거나, 너무 커진다.
    - sigmoid일때는 영향이 점점 약해진다. vanishing
    - relu일때는 영향이 점점 폭발적으로 증가한다. exploding

### LSTM
- long short term memory

- state
  - 중간에 흘러가는 컨베이어벨트
- gate
  - state로 가기위한 입구
  - 정보가 유용한지 아닌지를 판단,조작하여 올리는 입구


- previous cell state
  - t까지의 정보들을 요약
- next cell state

- previaou hidden state
- next hidden state

- output(hidden state)


- forget gate
  - 어느정보를 버릴지 결정
  - 현재의 입력(xt) + 이전의 출력(ht-1) -> ft 
- input gate
  - 현재의 입력이 들어왔을때, cell state에 어떤정보를 올릴지 정한다.
  - 현재의 입력(xt) + 이전의 출력(ht-1) -> it 
    - 어떤 정보를 버릴지 계산
  - 현재의 입력(xt) + 이전의 출력(ht-1) -> $$  \tilde{C_t} $$
    - cell state예비군
- update cell
  - cell state를 업데이트
  - Ct = ft*Ct-1 + it * $$ \tilde{C_t} $$
  - 이전 cell state(Ct-1)에 버릴정보(ft)를 통해 버리고, 예비 cell state($$  \tilde{C_t} $$)에 버릴정보(it)를 더하여 새로운 cell state(Ct)를 만든다.
- output Gate
  - 어떤 값을 출력할지 ht
  - 현재의 입력(xt) + 이전의 출력(ht-1) -> ot
  - ot와 다음으로 갈 update cell state(Ct)를 계산하여 출력(ht)를 만든다. 


### GRU
- gated recurrent unit
- 게이트가 2개
  - reset,update
- cell state가 없고 hidden state가 바로 사용
- LSTM과 비슷한성능, 성능이 더 높을 경우도 있다.
- transformer라는 또다른 구조가 더 좋은 성능이 있다고 한다.



## transformer
- sequential model
- 순서가 잘리거나, 변형이 되었을때를 해결하기 위해

### transformer
- 재귀적인 모델이 아닌, attention이라는 구조를 활용

- 기계어 번역에 사용됨
- 더 나아가 이미지 분류에 활용됨


- 어떤 번역할 문장을 입력받으면 원하는 언어로 번역하여 출력하는 구조
  - encoders, decoders가 있다.

- 한단어 단위로 rnn하는게 아닌 encoder는 한번의 n개의 단어를 처리한다
  - encoder와 decoder가 스택되어있다.
- encoder가 어떻게 한번에 처리하는게 가능한지?
- decoder가 어떻게 encoder된것을 새로운 언어로 변환이 가능한지?

### encoder
- self-attention, feed forward neural network로 구성됨
- self-attention
  - transformer의 핵심이라 한다.
  - 3개의 단어가 들어간다고 하면 3개의 feature vector로 나온다.
    - 각 feature에는 입력된 3개의 단어가 모두 고려된다.
- feed forward 는 독립적으로 지나간다.

- `the animal didn't cross the street because it was too tired`를 번역한다면?
  - it이 어느 단어에 의존되는지?
  - 모든 단어들을 고려하고 관계성을 보기때문에 animal이 관계가 높다고 학습이 되도록 해야한다.

- `thinking`, `machines`를 입력한다면
  - 입력된 값마다 queryis,keys,values라는 vector를 만든다.
  - score를 계산해준다.
    - `thinking`을 기준으로 하면 `thinking의` `qeuries`를 입력된 다른 단어들의 `keys`와 내적을 구한다.
      - 이는 attention의 원리와 같이 thinking이 다른 단어와 밀접한 연관이 있는지를 계산하게 된다.
    - 나온 score들의 값에서 normalized해준다.
      - 이때 나누는값은 key벡터의 몇차원을 하는지로 정하는 하이퍼 파라메터이다.
      - $$ \sqrt{d_k}$$
    - 이를 softmax
  - 이 score는 attention rate를 구할 수 있따.
    - 각 각의 단어와 얼마나 상호작용이 있는지를 나타낸다.
  - 이 softmax된 score값을 각 value에 곱을 해주고 더한다.
  - 이 더한값이 `thinking`이라는 입력의 encoding된 값이 된다.


- 특징
  - 옆의 단어가 달라지면 출력이 확 달라진다.
  - rnn은 n개의 단어만큼 반복적으로 처리하면된다.
  - transformer는 한번에 입력을 받나보니 어느정도의 한계가 있긴하다.
    - 하지만 그만큼 융통성있게 출력을 해준다.



- MHA
  - multi-headed attention
  - attention을 여러번한다.
  - 하나의 입력에 대해 key를 여러개 만들기
  - n번의 인코딩된 벡터를 구할 수 있다.
    - 입력과 출력을 맞춰줘야 한다.
  - (2,3)이 8개 나왔다면 (2,24)로 합치고, (24,5)와 곱하여 (2,5)로 만든다.



- positinal encoding
  - 순서가 보장되지않고 encoding되고있다.
  - 어떤단어가 먼저나오는지 표시해줘야한다.
  - 순서를 보장하도록 추가한다.


- encoder에서 decoder로 무엇이 전달되나
  - key와 value 벡터 가 넘어간다.
  - decoder는 정답과 넘어온 key,value를 받아서 학습을 진행한다.
  - test에는 입력된 key, value를 통해 단어를 만들게 된다.


- 이미지 처리에도 사용 가능하다.
- DALL-E
  - decoder를 사용했다.
  - 문장을 받아서 문자에 맞는 이미지를 출력한다.

----


## 좀더 찾아보기
- 독립동등분포
- MHA
- SDPA
- positional encoding

-----

## 피어세션 정리

### 조교님과의 질문 
- parameter norm penalty
  - 논문 추천
    - sharp thresholds for high-dimenstional and noisy recovery of sparity
    - a unified framework for high-dimensional analysis of estimators with decomposable reularizers
  - bias, variance
    - 파라매터가 너무 적으면 예측이 덜된다.
    - 파라메터가 너무 많으면 오버피팅이 된다.
  - detatch?
    - gradient를 추적한다는 메모리를 잡아먹는다.?
    - 추적은? 학습할때 gradient를 계산해야한다.
      - 역전파할때 추적이 사용된다고 한다.
  - correlation,convolution 차이
    - conv는 반대방향으로 계산된다.
      - 1,1과 3,3곱해서 진행
    - correlation은
      - 1,1과 1,1곱해서 진행
      - 우리가 알고있는 cnn의 진행

### 밋업
- 현재 공부를 못해도 시간을 들여서 공부를 진행하라
- 정리는 못하면 링크를 달아서 요약이라도 진행하라
- y절편도 중요하지만 기울기가 더 중요하다.


----

## 과제
- lstm
  - mnist로 진행
  - 가로 방향으로 잘라서 lstm으로 진행
  - lstm은 파라메터가 많다.
    - gate부분이 dense계산이 되어서
  - tensor곱은 matmul!
- SDPA,MHA