---
layout: post
title: 19.Transformer self-attention
categories: AI boost
tags: [ai,boost,NLP,Transformer,long term dependency,Scaled Dot-product attenction, fairseq,Multi-Head Attention,MHA, Layer Normalization, Positional Encoding,Learning Rate Scheduler, Masked Self-attention]
toc : true
math : true
---


# 강의 복습 내용
- Transformer(self-attention)
  - RNN기반 모델의 단점을 해결하기 위해 처음 등장
  - attention연산만을 이용해 입력 문장을 학습하며 학습속도가 빠르다는 장점이있다.

## 얻은 지식

-----

## Transformer
- RNN구조를 걷어내고 attention에만 집중
- 기존 rnn의 attention
  - 왼쪽에 나타난 정보가 오른쪽으로 점차 축적되면서 정보가 흐려진다.
- 양뱡향 rnn의 attention
  - 왼쪽부터 진행된 forward,오른쪽부터 진행된 backward를 결합
- 여기서 rnn이라는 순차적으로 진행되는 과정을 제외하고 attention만 집중


### long term dependency

![transformer사진](https://user-images.githubusercontent.com/24247768/108361620-13bebd80-7236-11eb-8fd1-8b15ae83b56d.png)
- 단어별로 입력x가 있고, 출력 h를 나온다.
- 따로 encoder와 decoder로 나뉘지않고 자기자신을 이용해 attention을 한다
- query벡터
  - 자기자신
- key벡터
  - 어느 정보를 가져올지 필요한 각각의 단어의 정보
  - query와 각 key들과 연산을 진행
  - 연산결과들은 softmax를 통해 가중치를 구한다.
- value벡터
  - softmax의 결과를 가중치로 사용한다.
- qeury는 하나이며, key와 value벡터는 각자 쌍으로 존재한다.

- 특징
  - 각 출력은 모든 입력을 고려하고 있다.
  - 멀리있어도 한번에 보면서 계산을 한다.
  - 멀리 있는 정보들도 rnn에서는 반복적으로 통과되면서 손실되는 한계를 개선한 모델이다.

###  Scaled Dot-product attenction
- 입력 
  - 하나의 query q
  - key-value쌍 (k,v)
- 출력 
  - value벡터의 가중평균
  - value벡터의 가중치는 query와 key의 inner product값
- query와 key는 같은 차원 d_k를 가져야한다.
  - 둘은 계산이 되야하므로
- value의 차원은 달라도된다.d_v
  - value는 나중에 가중치를 이용하므로
  - 최종적인 결과의 차원이다.

$$ A(q,K,V) = \Sigma_i{\frac{exp(q\cdot k_i)}{\Sigma_j{exp(q\cdot k_j)}} v_i} $$

- i번째 key로부터의 softmax를 지난 유사도를 구하는 식이 된다.


$$ A(Q,K,V) = softmax(QK^T)V $$

- 행렬식으로 정리

![행렬 그림](https://user-images.githubusercontent.com/24247768/108363139-f12da400-7237-11eb-8044-4ce8c16a6fa4.png)

- K는 transpose되어 Q와 내적이 되도록 한다.
- Q의 하나의 가로와 K의 세로를 통해 하나의 scala값을 구한다.
  - 하나의 Q에 대해 각 K마다의 연산을 진행, 하나의 row에 표현된다.
  - 이 결과는 row-wise softmax로 가로를 기준으로 1이되는 확률로서의 결과를 만든다.
- 나온 softmax의 하나의 row와 모든 V에 대해 가중치로 계산이된다.

![계산한 그림](https://user-images.githubusercontent.com/24247768/108363466-49fd3c80-7238-11eb-9b88-b581f2c8e425.png)
- Q와K^T가 계산된결과의 row한줄은 한단어가 각 단어마다의 가중치를 계산한다.
- 이 가중치를 V와 행렬곱을통해 V의 세로줄로 행렬곱이 되면서 해당 가중치만큼의 결과를 구할 수 있다.
  - 헷갈리면 직접 그려보면 됨

- 문제점
  - query와 key의 차원에 따라 분산이 커질수있다.
  - 분산이 커지면 특정부분에 집중될 수 있으므로 이를 scalling해줘야한다.
  - 평균은 0, 분산은1로 만드는 과정이 필요
  - 이를 위해 루트 d_k로 나눠준다.

$$ A(Q,K,V) = softmax(\frac{QK^T}{\sqrt{d_k}})V $$


![참고그림](https://user-images.githubusercontent.com/24247768/108297975-7d5fad00-71df-11eb-9fc2-cbbd0e298043.png)


### Multi-Head Attention
- 좀더 확장된 transformer
- 입력에 대해 여러 버전의 attention을 진행
  - Multi-Head Attention
- attention결과를 concat한다.

![mha그림](https://user-images.githubusercontent.com/24247768/108366389-a9108080-723b-11eb-9a91-7d27a3b0fb73.png)

$$ MultiHead(Q, K, V) = Concat(head_1, \cdots , head_h)W_O $$

- W_O를 통해 concat한결과를 출력값으로 나타낸다.

$$ where \, head_i = Attention(QW_i^Q ,KW_i^K ,VW_i^V )$$

- 각 i번째 head는 Attention을 구한다.
  - 이런 head가 여러개라는 소리


- 왜 사용하는지?
  - 서로 다른 기준으로 여러 측면에서 정보를 가져와야 하는 경우가 생김


![concat그림](https://user-images.githubusercontent.com/24247768/108366944-48ce0e80-723c-11eb-858f-9f9c6d19a2bb.png)

- 여러개의 head로부터 나온 결과 Z0~7이 나온다.
- 이들을 concat하여 하나의 행렬로 만든다.
- 원하는 행렬Z를 얻기위한 W_o를 계산한다.

### 복잡도로 보는 rnn,self-attention 정리

- 레이어별 복잡도
  - self attention 
    - o(n^2d) - 각 Q마다 모든입력 n개의 K들을 연산하므로
  - recurrent
    - o(nd^2) - ht-1에서 ht로 진행될때 whh는 (d,d)이고 ht-1은 (d,1)이되어 계산되어 d를 d번 계산하는 방식이 된다고 한다.
  - d는 hidden state의 차원으로 우리가 임의로 정해주지만 n은 입력길이다.
    - self attention이 좀더 메모리를 요구한다.
- sequentioal operations
  - self attention 
    - o(1) - 한벙의 입력으로 바로 출력, 학습이 빠르다.
  - recurrent
    - o(n) - 무조건 한단어 사용되고 나서 계산이 일어나므로 병렬화가 불가능
- Maximum path length
  - self attention 
    - o(1) - 한번에 학습하므로 아무리 멀어도 필요한 만큼 정보를 바로 얻을 수 있다.
  - recurrent
    - o(n) - 맨끝의 n번째 단어는 1번째 단어를 참고하기위해서는 n번의 계산을 거친것을 통해 본다.


###  Block-Based Model
- MHA를 핵심으로 하여 후처리하는 모듈?

![그림](https://user-images.githubusercontent.com/24247768/108367357-ba0dc180-723c-11eb-9a4b-4510ba0883af.png)

- 입력이 V,K,Q가 들어간다.
- MHA를 수행
- resnet과 같은 residual connection을 사용하였다.
  - 추가적으로 ADD & Norm을 수행한다.
- 이후 Feed Forward를 통과하고, Add & Norm을 수행
- Add는 입력과 출력을 더해준다.
- 왜 FF가 있나?(나의 생각)
  - 기존의 RNN의 문제를 해결하기 위해 FeedForward에 Attention을 넣은게 Transformer
  - Attention은 문장의 관계와 연관들을 알기위해 추가적으로 넣은 모듈로 생각함


### Layer Normalization
- 여러 normalize가 있다.
  - batch, layer, instance, group

- layer normalization은 입력에 대해 평균은 0, 분산은 1로 통계값을 바꾸는것
  - 너무큰차이를 평준화하는것
- 차원이 커지면 분산이 커진다
  - 2차원에서 3차원이 되면서 하나의 차원때문에 사이의 거리가 늘어나게 된다.
- layer normalization은 각 입력마다 진행을 한다.
  - batch normalization은 각feature

![그림]
- [논문](https://arxiv.org/abs/1607.06450)
- [BN과LN](https://yonghyuc.wordpress.com/2020/03/04/batch-norm-vs-layer-norm/)

###  Positional Encoding
- 순서가 없다.
- i go home과 home go i 가 차이없이 계산된다.
- sin과 cos같은 주기를 이용하여 해당position만의 특정한값을 만들어서 사용.
  - 하나의 key값과 같이 결정, 매번 다른값이 나오지않는다.

![그림](https://user-images.githubusercontent.com/24247768/108369980-81231c00-723f-11eb-9f6d-1de4d3e404e2.png)

- [참고](https://skyjwoo.tistory.com/entry/positional-encoding%EC%9D%B4%EB%9E%80-%EB%AC%B4%EC%97%87%EC%9D%B8%EA%B0%80)

###  Warm-up Learning Rate Scheduler
- learning rate도 중간에 수정되면서 적절히 학습을 하도록 하는것
- 초반은 lerning rate를 적게
- 점차 learning rate를 점차 증가하다가 다시 감소하는 형태를 보인다.

![러닝레이트](https://user-images.githubusercontent.com/24247768/108370126-a57ef880-723f-11eb-85e2-6f4e3eb58df0.png)

### High-Levl View
- Transformer의 전체적인 구조

![그림](https://user-images.githubusercontent.com/24247768/108370240-c47d8a80-723f-11eb-8d8c-fdf0800dde64.png)


- encoding쪽은 N번 해당 블럭을 반복적으로 진행된다.
- 이 결과물을 Decode에 사용된다.


### Encoder Self Attention Visualization

![그림](https://user-images.githubusercontent.com/24247768/108370388-f1ca3880-723f-11eb-8b22-367eb6f2bf67.png)

- 위쪽은 입력한 단어들
- 아래는 MHA의 결과인 attention되는 것을 색으로 표현되고있다.
  - row는 MHA의 head, column은 각 출력될 단어들
  - 한줄씩보면 각 head가 어느 단어에 attention하는지 확인이 된다.
  - 일부 head는 자기자신을 attention하기도 하는것을 본다.

- [링크](https://colab.research.google.com/github/tensorflow/tensor2tensor/blob/master/tensor2tensor/notebooks/hello_t2t.ipynb)

### Decoder

![그림](https://user-images.githubusercontent.com/24247768/108371087-bb40ed80-7240-11eb-8c17-45fad0dad2af.png)

- ground truth를 sos토큰으로 시작하여 masked MHA를 진행한다
- decoder에서 만든것은 Q로 encoder의 결과를 V와K로 입력을 하는 MHA를 진행한다.
  - encoder vector중에 어느것을 사용할지를 attention하는 과정을 가진다.
- 이후 추가적인 Feed Forward를 진행한다.

### Masked Self-attention

![그림](https://user-images.githubusercontent.com/24247768/108371150-cf84ea80-7240-11eb-9ccb-d0bc2a81a045.png)

- 입력이 "SOS", "나는", "집에"
- 예측으로 각각 "나는", "집에", "간다" 가 된다.
- self attention을 통해 encoding하는 과정에서 정보의 접근 가능 여부에 관련있다.

![그림2](https://user-images.githubusercontent.com/24247768/108371184-d6abf880-7240-11eb-8580-fa1f301d1692.png)

- self attention으로 query와 key를 softmax로 계산
- 결과물은 sos일때 sos,나는,집에라는 단어 각각 얼마정도의 유사도를 가질지의 정보를 가진다.
- 하지만
  - 각 단어가 나머지 모든 단어에 대해 정보를 접근이 가능하면
  - sos만 주어졌을때 학습데이터로 나는,집에가 주어져야 하지만,
    - 예측으로는 sos로부터 나는을 예측해야 한다.
    - sos시에는 나는,집에는 접근이 안되도록 가려줘야한다고 한다.

![그림](https://user-images.githubusercontent.com/24247768/108371273-edeae600-7240-11eb-8a06-d95e449c1b11.png)

- 현재 위치이후 나오는 결과들을 전부 0으로 하여 학습
  - 현재 단어와 이전단어는 볼 수 있고, 다음 나올 단어는 보지 못하게 한다.


----

## 좀더 찾아보기
- Transformer
  - [참고사이트](https://ahnjg.tistory.com/57)
  - [참고사이트2](https://medium.com/platfarm/%EC%96%B4%ED%85%90%EC%85%98-%EB%A9%94%EC%BB%A4%EB%8B%88%EC%A6%98%EA%B3%BC-transfomer-self-attention-842498fd3225)
- masked MHA
- Positional Encoding
  - sin과 cos주기가 무슨 의미인지?
- label smoothing
  - cnn에서는 이미지를 일부 자르거나 붙이는 과정
  - 뭘까?

-----



## 피어세션 정리
- 어제 과제
  - batch사이즈변경
    - batch늘리는거랑 lr을 줄이는거랑 비슷한효과
  - epoch증가
  - criterion의 label_smoothed_cross_entropy와 label_smoothed_cross_entropy_with_alignment가 무엇인지?
    - alignment로 바꾸니 성능이 올랐다. 왜 오르는지?
- position
  - 벡터에서 특정 정보가 위치를 알기위해 상수를 더해준다고 한다.
  - 위치에 맞는 값을 해줘야 한다.
  - sin, cos는 계속 바뀌는것
  - 이들을 조합해서 해당위치만의 특별한 값을 만들어서 사용한다는 의미라고 생각





### further question
- attention은 이름 그대로 어떤 다넝의 정보를 얼마나 가져올지 알려주는 직관적인 방법처럼 보인다.
  - attention을 모델의 ouput을 설명하는데에 활용할 수 있을까?
  - 지금까지 설명한건 out도 같이 보면서 설명해줬기때문에 
  - 논문 확인해보기
    - [Attention is not Explanation](https://arxiv.org/pdf/1902.10186.pdf)
    - [Attention is not not Explanation](https://www.aclweb.org/anthology/D19-1002.pdf)

------

## 실습
- MHA
- Masked MHA

------

## 과제 설명 fairseq
- fairseq 라이브러리 익숙해지기
  - command-line tool를 통해 학습하는 방법 공부
- library reference에는 model로 여러 모델이 제공된다.
- 세부적인 모델조작과 데이터 처리가 어렵다.
- 모든 argument를 설명해주지 않아서 애매하다고한다.
- 기타 라이브러리
  - [openmnt](https://openmnt.net)
  - [huggingface](https://github.com/huggingface/transformers) 
    - 좀더 잘 정리되어있다고 한다.
    - example을 참고해서 진행

### Hyper parameter Tuning in pytorch
- model free hyperparameter
  - learning rate, batch size, training epoch
  - learning rate scheduler, optimizer
  - weight initialization
  - ealy stop strategy
  - regularization
  - dropout
  - perturbation or noise of an input
- model
  - kernel size
  - layer, hidden units, embedding units 갯수
  - pooling
  - activation function
- 전부고려하면 엄청많음
- 기타
  - accumulation step
    - 한번 학습하는 batch가 적으면 학습이 잘안된다.
    - 16batch, 4batch를 4accumulation성능이 다를때가 있다.
    - batch-wise operation
  - random seed
    - 특정 seed가 있다.
    - mrpc,rte,cola,sst
  - number of evaluation
- random search
  - 랜덤하게 표현
  - grid보다 좀더 좋다.
- 최적화
  - grid search
    - 일정한 간격으로 
  - learning late 
    - fine-grained

