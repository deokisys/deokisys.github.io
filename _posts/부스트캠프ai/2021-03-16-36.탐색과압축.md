---
layout: post
title: 36.탐색과압축
categories: AI boost
tags: [ai,boost,hyperparameter search, NAS, neural architecture search,cross entropy]
toc : true
math : true
---

# 강의 복습 내용
- 공간과시간
  - 시간과공간 복잡도
  - hyperparameter search
    - surrogate model
  - neural architecture search(NAS)
- 알뜰히
  - 인코딩
  - 디코딩
  - 압축
## 얻은 지식

-----

## 모델의 시간복잡도
- 5가지의 기본 동작
  - inputting, processing, outputting, controlling

### 공간과 시간
- 공간
  - solution space, search space
    - 어떤 문제를 해결하기위한 candidate?모아놓은 집단
  - problem space
    - 문제가 정의된 공간
- ![그림](https://user-images.githubusercontent.com/24247768/111448500-d163a000-8751-11eb-8409-8f83c9d92f58.png)
  - 사이클 만들지 않고 모든 영역 거쳤는지
  - 빨간박스로 되어있는게 solution space
    - 모든영역을 거친 정답이되는 집단
  - 그외는 problem spce에 있지만, solution space에 있지 않은 것들
- 시간
  - P
    - 결정론적 튜링 기계로 다항시간안에 풀수있는 판정문제 
  - NP(Non deterministic plynomial)
    - 비결정론적 튜링 기계로 다항시간 안에 풀 수 있는 판정 문제
    - 머신러닝이 쓰이는 영역
    - 시간이 오래걸린다고 생각
- 시간 공간 trade off
- 엔트로피
  - 미래에서 과거로
    - 정렬이 안되는 상태에서 정렬된 상태로
  - low->high->low라는 관점?
  - low는 정렬된상태(초기,완료된상태)
  - high는 정렬이안되는 무질서한 상태

### parameter search
- 하이퍼 파라메터를 조절해주는게 점점 줄어든다
  - 내부 파라메터는 기계가 직접 찾아야한다.
- loss를 줄이는 방식으로 parameter를 찾는다.
  - 기울기 하강법
  - 글로벌 보장이 안됨

### hyper parameter search
- 한번 바꾸는데 많은 시간과 비용이 든다.
  - 파라메터를 다시 잡아줘야한다.
- exploitation
  - 하이퍼파라메터 잡고 다시 모델을 학습
- exploration
  - 어떤 하이퍼 파라메터를 쓸지 선택
- ![그림](https://user-images.githubusercontent.com/24247768/111448595-ec361480-8751-11eb-831e-c82b52b9dec4.png)
  - manual, grid, random search방식이 존재
- surrogate model
  - gaussian process
  - 대리자라는 뜻
  - ml model에 대한 hyperparameter(x,f(x))세트를 가진다.
    - 이 hyperparameter에 대한 세트를 학습

### neural architecture search(NAS)
- vgg,resnet 등등에서 찾기
- 여러 후보군에서 전부 돌려보기
- 몇층할지, conection을 어디에 할지

### NAS for edge devices
- MnasNet
  - 모바일에 맞춘 acrchitecture search
- proxylessnas
  - 이전엔 proxy로 일부를 대신 해주는 방식을 했다고한다.
  - 이런 proxy없이 했다고한다.
- once-for-all
  - 하나를 만들고 다른것에 적용하도록
- regular convolution, depth-wise separable convolution

-----

## compression
- 압축
- mincode
  - dic을 이용하여 문자를 압축

### 압축
  - 비손실, 손실로나뉨
  - 복원시 손실이되냐 안되냐
- 허프만 코드
  - 많이 사용되는 단어는 적은비트, 적게사용되는 단어는 좀더 큰 비트로 저장

### 부호
- coding
- 프로그램밍- 컴파일 - FSM을 만든다
  - 이러한 구조가 languagage를 통해 이루어진다.
- 모델
  - 입력, 모델이 인코딩(부호화)를 통해 출력을 만든다.

### 부호화
- encoding
- cross entropy
  - q라는 codebook을통해 p라는 메시지를 해독


### 압축률
- compression rate
  - 파라매터수/압축한 파라메터 수
- space saving rate
  - (파라매터수-압축한파라메터수)/압축한 파라메터 수
- speedup rate
  - 실행시간/압축한 실행시간



----

## 좀더 찾아보기
- 엔트로피란?

-----


## 피어세션 정리
- 질문
- 엔트로피?
  - 하나의 분포
  - a-z에서 p와 a-p에서 p의 정보량은 a-z가 더 크다.
- 크로스엔트로피
  - 두 분포를 이용해 정보량을 계산
- [엔트로피참고](https://hyunw.kim/blog/2017/10/14/Entropy.html)
- [엔트로피참고2](https://stats.stackexchange.com/questions/265966/why-do-we-use-kullback-leibler-divergence-rather-than-cross-entropy-in-the-t-sne)


