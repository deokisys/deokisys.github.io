---
layout: post
title: P-S1-4.학습과 추론
categories: AI boostP
tags: [ai,boost,모델,train]
toc : true
math : true
---

## 강의확인

### training과 inference1
- Loss
- Optimizer
  - loss를 어떻게 업데이트를 할지?
  - lr 스케줄러(학습할때 lerning rate를 조절하는것)
    - `torch.optim.lr_scheduler.StepLR()`
      - 특정 step마다 감소
    - `torch.optim.lr_scheduler.ConsineAnnealingLR()`
      - 코사인 함수 혀애로 LR을 변경
    - `torch.optim.lr_scheduler.ReduceLROnPlateau()`
      - 성능향상이 없을때 LR을 감소
- Metric
  - 학습에 영향을 미치진 않는다.
  - 모델의 평가
  - accuracy,f1-score,precision,recall
  - MAE,MSE
  - MRR,NDCG,MAP
  - competition에서는 정해주지만 실무에서는 직접 선택해야한다.
    - 어떤 metric을 사용해야할지를 알아야한다.
  - 

### training
- `model.train()`
  - 학습모드로 전환!
- `optimizer.zero_grad()`
  - 배치마다 grad를 초기화
  - 이전 배치에서 사용된 grad를 안쓰기위해 사용
- `loss = criterion(outputs,labels)`
  - `criterion = torch.nn.CrossEntropyLoss()`
    - criterion이라고 일반적으로 선언
  - 모델에 input을 넣고 forward를 진행하였을때, 각 각 연결된 모듈에 있는 grad_fn이 적용되면서 chain이 적용된다.
  - `loss.backward()`는 이러한 grad_fn이 체인되어있는것을 거쳐가면서 계산을 진행한다. 
  - loss는 grad를 업데이트하기만한다. 이를 파라메터를 적용하진않는다.
    - 이를 적용해주는게 optimizer
- `optimizer.step()`
  - 계산된 grad를 파라메터에 적용해준다.
- gradient accumulation
  - 매번 배치마다 업데이트하기엔 부담이될때
  - 하나의 배치에서 loss를 계산하고 optimizer.step과 optimizer.zero_grad를 안한다.
  - 다음 배치에서 loss를 계산, step을 진행하면 중첩적으로 계산하게된다. 이후 zero_grad로 초기화해준다.
    - 이렇게하면 2번의 배치마다 업데이트가 적용된다.

### inference
- `model.eval()`
  - 평가로 전환!
- `with torch.no.grad():`
  - 검증에서는 계산과 업데이트가 필요없다.
  - 조건문처럼 감싸서 진행
- 그외에 저장을 원하면 torch.save를 한다.

### pytorch lightning
- 간단하게 사용하려고 만든거?
  - keras처럼 fit으로 바로 학습을한다.
- 어느정도 머신러닝 프로세스를 알고서 해야 좋을것이다!

-----

## 피어세션
- 데이터를 분리해서 모델을 따로 학습해서 앙상블할지 고민한다.
  - 3개로나누기
  - 3개로 나누면 확장성면에서 문제가 있지않을까 생각하심
    - 모델은 한개면 충분할듯하다.
- 데이터 불균형은?
  - 부족한건 augmentation적용하려고한다.
- mnasnet,shufflenet,inception이 좋았다.
- 다들 너무 잘해서 뭘 말할게 없었다...

## 오피스아워
- 설정바꾸는 백그라운드필요
  - 모델,loss, optimizer, loging등을 바로 바꿔서 바로 볼 수 있도록 베이스라인이 필요
- label smoothing, mixup과 같은 데이터 변형해보기


-----

## 오늘 한일
- 전체적으로 학습하는과정 적용해보기

## 어떻게 했는지
- 오늘 배운 과정을 한번 확인해보기위해 함수를 적용하여 학습을 해보았다.
  - train을 위해 optimizer와 loss를 사용하여 epoch마다 계산되도록 적용
  - 이전에 작성한 dataset과 model을 사용해보았다.

## 좋았던 점
- 피어세션에서 다들 고민하던 부분을 하는 것을 알 수 있었다.


## 아쉬운 점
- 피어세션에서도 데이터 분석이 더 중요하게 보였다.
  - 아직 데이터부분을 제대로 다루지 못해서 일단 집중적으로 진행해야겠다고 생각했다.
- 데이터를 보는것과, augmentation하는것, 불균형을 해결하기위해 어떤 전략을 할지가 부족하다.
- 다들 학습하는것을 잘 다루는데 나는 못해서 피어세션에 끼지 못하였다.



