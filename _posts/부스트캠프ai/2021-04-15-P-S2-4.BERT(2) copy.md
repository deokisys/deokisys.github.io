---
layout: post
title: P-S2-4.BERT(2)
categories: AI boostP
tags: [ai,boost]
toc : true
math : true
---

## 강의확인

### BERT학습1
- 토크나이저 만들기, 데이터셋 확보, NSP(next sentence predictino), masking과정이 필요
- pretrained도 성능 좋지만 도메인 특화로 학습을 새로 진행하면 성능이 더 좋다!
- dataset- 모델이 학습하기위한 밥, dataloader - 모델에게 어떻게 넘길지
- 코퍼스-데이터셋-데이텅로더로 가져오기

### 실습
- BERT개인정보 관련 조심  
  - mask된 부분을 예측하는데 특화되어있다.
  - 개인정보를 mask를 통해 예측가능
  - 개인정보가 해소된 데이터로 학습을 진행해야 한다.
- bert학습위한 토크나이저 만들기
  - 한국어 wiki를 이용
  - wordpiece tokenizer를 진행
    - 만들때 special토큰을 선언하는것은 [mask]라는것을 하나의 토큰이라고 선언해줄뿐 실제 스페셜토큰으로 인식을 안하는것같다.
    - 이후 add_special_tokens에서 dict형태로 {'mask_token':'[MASK]'}라고 해줘야 해당 토큰이 mask라는 기능을 수행하도록 부여하는것같다.
      - 둘은 다른 객체 
- bert껍데기 만들기
  - `config = BertConfig()`를 통해 설정
    - [참고](https://huggingface.co/transformers/model_doc/bert.html#bertconfig)
    - vocab_size - tokenizer만든것과 동일하게 수정
    - hidden_size - hidden vector크기
    - num_hidden_layer - hidden layer를 몇개쌓을지
      - 빠르게는 3-4개로 지정한다고 한다.
    - num_attention_heads - attention head 갯수
    - intermediate_size - feed-forward가 존재, 이들의 dimension 크기를 지정
    - max_position_embedding - embedding 사이즈로, 최대 몇개의 token을 입력으로 받을지 지정
      - task에 따라 조절
    - type_vocab_size - type id의 범위(?)로 bert는 segmentA, segmentB로 2종류로 기본 2이다.
  - `BertForPreTraining(config = config)`로  위에서 만든config를 통해 모델을 선언
- 데이터셋 만들기
  - input ids, token type ids, next sentense prediction(NSP)가 있어야한다.
  - document(문서)단위로 진행
  

-----


### 피어세션
- 허깅페이스 허브 실습
- pretrained모델 가져오는거
  - epoch 60이상해보았다. 
  - `bert_config = transformers.BertConfig.from_pretrained(MODEL_NAME)`
  - `bert_config.num_labels = 42`로 설정해주기
  - `model = transformers.AutoModelForSequenceClassification.from_pretrained(MODEL_NAME,config = bert_config)`
  - 2000step
- MASK을 왜 add_special_tokens를 하는이유?
  - tokenized_text와 tokenizer는 다르다.
  - 둘은 다른 객체로 마치 껍데기와 실제 가져오는 것으로 나뉘므로 따로 설정해야 하다.


## 오늘 한일
- bert train해보기
- 허깅페이스 허브 접속, 모델 업로드
- competition 제출


## 어떻게 했는지
- 데이터셋 정의하고, 학습을 진행
  - dataset을 통해 "input_ids", "token_type_ids","next_sentence_label"를 만들어낸다.
  - 학습시에는 bertconfig를 통해 학습관련 설정을 진행
- 허깅페이스에 git과 같은 형식으로 모델 업로드
  - 업로드된 모델 불러오는것을 실습
- competition은 베이스코드를 실행하여 제출


## 좋았던 점
- 어느정도 학습을 진행하는것을 알 수 있었음
- 허깅페이스를 불러오는것과 사용하는방법을 알 수 있었음



## 아쉬운 점
- 허깅페이스 허브에서 불러오긴하느데 실제로 사용하려니 아직 제대로 동작은안함
  - 단순히 불러와서 사용하는게 안됨
  - 모델을 업로드할때 따로 설정해주거나, 다운로드해서 사용할때 추가적으로 진행해야하는게 있어야 할거같다.
  - 좀더 확인이 필요
